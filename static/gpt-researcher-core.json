{
    "ANKIConfig": {
        "GIT_URL": "https://github.com/assafelovic/gpt-researcher/blob/master/"
    },
    "gpt_researcher__master__agent__GPTResearcher": {
        "label": "GPTResearcher",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 15,
        "endLineNo": 574,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L15-L574&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "],\n```python\nasync def get_similar_written_contents_\n   async def __get_similar_written_contents_by_query(\n        self,\n        query: str,\n        written_\ncontents: List[Dict],\n        similarity_threshold: float = 0.5,\n        max_results: int = 10\n    ) -> List[str]:\n        if self.verbose:\n            await stream_output(\n                \"logs\",\n                \"fetching_relevant_written_content,\n                f\"\ud83d\udd0e Getting relevant written content based on query: {query}...\",\n                self.websocket,\n            )\n\n        #  Retrieve similar written contents based on the query\n        # Use a higher similarity threshold to ensure more relevant results and reduce irrelevant matches\n        written_content_compressor = WrittenContentCompressor(\n            documents=written_contents, embeddings=self.memory.get_embeddings(),\n            similarity_threshold=similarity_threshold\n        )\n        return await written_content_\n             query\n        async def process_query(query: str) -> Set[str]:\n            return set(await self.__get_similar_written_contents_by_query(query, written_content\n         query: str,\n         written_contents: List[Dict],\n         similarity_threshold: float = 0.5\n         max_results: int = \n         \nclass Your\n    def __init__(self,\n    self.headers\n        self.headers=header\n        self.\n        self.headers or {},\n    self.role\n    self.cfg = Config(config_path),\n        # If specified, the researcher will use the given urls as the context for the research task.\n        self.source_urls: str = None,\n        self.documents: None,\n        self.memory = Memory(self.cfg.embedding_provider\n    self.visited_urls = set(),\n        self.verbose: bool = True,\n    \n        self.websocket = websocket,\n        self.agent = agent,\n        self.context = context,     self.role\n         self.headers = headers or {},\n    self.report_source\n        self.report_type = report_type,\n        self.tone = Tone.Objective\n            se\n        self.cfg = Config(config_path=config_path\n         self.visited_\n\n    def __init__\n    def __init__(self,\n        self.agent: str = None,\n        self.role: str = None,\n        self.report_type: str = ReportType.ResearchReport.value,\n        self.agents: str = None,\n        self.\n        self.documents = documents,\n        self.reportsourc\n        self.parent_query: str = \"\",\n        self.subtopics: list = [],\n        self.visited_url\n        self.sources = None,\n        self.document\n        self.report_source\n        self.voice\n  await\n        async def documents\n        self.memory = \n        self.verbose = self.\n        self.websocket\n        self.agents =\n        self.role = \"default_role\n        self.report_type = \"research_report\n        self.source_url = None,\n        self.documents\n        self.report_source\n        self.reportsource = self.source\n        self.agent = None\n        self.report\n        self.memory\n        self.headers = {},\n\n    async def conduct_research(self,\n            await\n        self.agent\n        self.context\n        self.source_\n        self.tone = Tone.Objective,\n        self.headers= self.headers,\n        self.context = context,\n\n    async\n        self.memory\n        self.\n    self.report_source\n        self.reports =\n        self.document\n        self.role = role,\n        self.history_report\n        self.\n        self.report_\n        self.\n        self.documents = documents,\n        self.memory = Memory(self.\n        self.\n    def write/get_\n    def __init__\n    self.report_ type = ReportType,\n        self.\n        self.report_source\n        self\n        self.memory = Memory(self\n        self.agent = agent,\n        self.report\n\n            self.agent =\n        self.report_\n        self.context\n            self.memory\n            self.report_source\n            self.document_source\n            self.report_source\n        self.report\n        self.memory = None\n\n    \n        self.\n\n```\n\n\n\n        await\n        self\n        self.tone\n   \n\n    async def __init__\n        self.report_\n        self.report_source\n        self.report\n        self.document\n        self.agent.\n        self.\n        self.context = None\n        self.role = None\n\n    async def __init__\n    \n    def __init__\n    self.report\n\n    def __init__\n    def __init__\n        self.report\n        self.report_\n        self.report\n\n\n\n    \n\n    \n\n    \n    def __init__\n    self.report\n            self\n            self.report_\n        self.\n    def write_\n    def __init__\n\n\n\n        self.report_\n            self.report\n            self.report\n            self.agent\n            self.report_\n        self\n            self.rep\n            self.report_\n            self.repor\n            self.report\n            self.\n            self\n\n\n\n        self.report\n        self.report_\n        self.rep\n            self.report\n            self.report\n            self.report_\n            self.report\n            self.report_\n        self.report_\n            self.report\n            self.report\n            self.report_\n            self.report_\n            self.report\n            self.report_\n\n\n\n    \n\n    async def __init__\n            self.report\n            self.report\n            self.report_\n\n            self.report_\n\n\n\n    \n            self.report_\n\n\n\n            self.report_\n             self.report\n            self.report_\n            self.report_\n            self.report\n\n\n\n            self.report\n            self\n            self.report_\n\n    async def write_\n            self.report\n            self.report_\n            self.report_\n            self.report_\n            self.report\n            self.report_\n            self.report\n            self.report_\n\n\n\n    def __init__\n            self.report\n            self.report_\n            self.report\n            self\n            self.report_\n\n      st\n            self.report_\n            self.report\n            self.report\n            self.report\n            self.report\n            self.report\n            self.report_\n            self.report\n            self.report\n            self.report_\n            self.report_\n            self.report_\n\n\n\n    def __\n           self.report\n            self.report_\n            self.report\n            self.report\n            self.report_\n            self.report_\n            self.report_\n            self.report_\n            self.report_\n            self.report_\n            self.report\n            self\n            self.report\n\n            self.\n            self\n            self.report\n            self.report\n            self.report_\n            self.report\n            self.report_\n            self.report_\n            self.report_\n            self.report\n            self.report_\n            self.report\n            self.report_\n            self.report\n            self.report_\n            self.report\n            self.report_\n            self.report\n            self.report_\n\n\n            self.report_\n            self.report_\n\n    async def __init__\n            self.rep \n            self.report\n            self.report\n            self.report_\n\n\n            self.report\n            self.report_\n            self.report_\n            self.report\n            self.report\n            self.report_\n            self.report\n            self.report\n\n\n\n    async def __init__\n    async def __init__\n            self.report\n\n    \n            self.report\n\n    async def __init__\n            self.report\n            self.report\n            self.report_\n            self.report\n            self.report\n            self.report\n            self.report\n            self.report\n            self.report_\n\n    \n            self.report_\n            self.report_\n            self.report_\n            self.report_\n            self.report\n            self.report\n            self.report_\n            self.report\n            self.report\n            self.report_\n            self.report_\n            self.report\n            self.report_\n            self.report_\n            self.report_\n            self.report\n            self.report_\n            self.report_\n            self.report\n            self.report\n            self.report\n\n\n\n    AS\n            self.report\n\n    async def __init__\n\n    async def __init__\n\n\n\n## Con\n            self.report_\n            self.report\n            self.report_\n            self.report\n\n            self.report_\n            self.report_\n\n    async def __init__\n            self.report_\n\n    async def __init__\n            self.report_\n            self.report_\n            self\n            self.report_\n            self.report_\n            self.report\n            self.report_\n            self.report\n            self.report\n            self.report_\n            self.report\n            self.report\n    async def __init__\n            self.report_\n            self.report\n            self.report_\n            self.report\n            self.report\n            self.report_\n            self.report\n            self.report_\n\n\n\n    \n\n    async def\n            self.report_\n            self.report_\n            self.report_\n            self.report\n            self.report_\n            self.report_\n\n\n     async\n            self.report\n            self.report\n            self.report\n            self.report_\n            self.report_\n\n\n    def __init__\n\n\n\n    \n            self.report_\n            self.report_\n            self.report_\n\n\n\n    async def\n            self.report\n            self.report_\n            self\n            self.report_\n\n\n\n            self.report\n            self.report_\n            self.report_\n            self.report_\n\n\n\n    \n\n    async def __init__\n\n\n\n    async\n            self.report\n            self.report_\n            self.report_\n\n    async def __init__\n\n    async def __init__\n            self.report_\n\n\n\n    \n            self.report_\n            self.response\n\n    \n\n\n    async def __init__\n            self.report\n            self.report\n\n    \n\n\n    \n\n    async def __init__\n\n\n\n    \n            self.report\n            self.report_\n            self.report_\n\n\n\n\n    async defer\n\n\n\n    \n            self.report\n            self.report_\n\n\n\n    \n            self.report_\n\n\n\n    \n            self.report_\n\n\n\n    \n\n\n    async def __init__\n            self.report_\n            self\n            self.report_\n            self.report_\n            self.report_\n\n\n\n    \n            self.report\n            self.report_\n\n\n\n    async def __\n            self\n\n    async def __init__\n\n\n    \n            self.report\n            self.report_\n\n\n\n    \n            self.report_\n\n\n\n    \n            self.report_\n\n\n\n    \n\n    \n\n    \n\n    \n\n\n\n\n\n\n\n\n        self.report_\n\n\n\n    \n\n    \n            self.report\n            self.report\n            self.report_\n\n\n\n    \n            self.report_\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report_\n\n\n\n    \n            self.report\n\n    \n            self.report\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self\n\n    \n\n\n    \n            self.report\n\n\n\n    \n            self.report\n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self\n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n\n\n    \n            self.report\n\n\n\n    \n            self.rep\n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.re\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.reports\n\n\n\n    \n            self.report\n\n\n\n    \n\n\n    \n            self.report\n\n    \n\n\n\n    \n            self.report\n           \n    \n\n\n    \n\n\n    \n\n\n    \n            self.report\n\n\n\n    \n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n\n    \n            self.report\n\n\n\n    \n\n\n    self.report\n\n\n\n    \n            self.report\n\n\n\n    \n           \n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.rep\n            self.report\n\n\n\n    \n            self\n            self.report\n\n\n\n    \n\n            self\n\n\n\n    \n            self.report\n\n\n\n    \n\n    \n            self.report\n\n```\n\n    \n            self.report\n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n\n```\n\n    \n\n    \n\n    \n           \n            self.report\n\n\n\n    \n\n    \n            self.report\n\n\n\n    \n            self\n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.\n            self.report\n\n\n\n    \n```\n\n\n\n    \n            self.report\n\n\n\n    \n            self\n            self.report\n\n\n\n    \n            self.report\n\n    \n            self\n\n\n\n    \n            self.report\n\n\n\n    \n            self\n\n    \n\n    \n            self.report\n\n\n\n    \n            self\n            self\n\n\n\n    \n            self\n            self\n\n```\n            self\n            self.\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self\n           \n\n\n    \n\n    \n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n\n\n\n    \n            self.\n            self.report\n\n\n\n    \n            self.report\n\n\n\n    \n            self.\n            self.\n            self.report\n\n\n\n    \n            self"
    },
    "gpt_researcher__retrievers__pubmed_central__pubmed_central__PubMedCentralSearch": {
        "label": "PubMedCentralSearch",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/pubmed_central/pubmed_central.py",
        "relativePath": "gpt_researcher/retrievers/pubmed_central/pubmed_central.py",
        "lineNo": 7,
        "endLineNo": 173,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fpubmed_central%2Fpubmed_central.py%23L7-L173&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## PubMedCentralSearch Retriever Analysis\n\n**Quick Summary**\n\nThis code defines a class `PubMedCentralSearch` that searches PubMed Central for articles matching a given query, retrieves the full text for those articles, and extracts key information like title, abstract, and a snippet of the body. It helps users efficiently access and process research articles from PubMed Central.\n\n\n**Inputs**\n\n*  `query`:  A string representing the search query for articles.\n*  `max_results`: An integer specifying the maximum number of results to return (defaults to 10).\n\n**Output**\n\n*   A list of dictionaries, where each dictionary represents a search result and contains:\n    *   `href`: A URL link to the article on PubMed Central.\n    *   `body`: A snippet of the article's body text (up to 500 characters). \n\n\n"
    },
    "gpt_researcher__llm_provider__generic__base__GenericLLMProvider": {
        "label": "GenericLLMProvider",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/llm_provider/generic/base.py",
        "relativePath": "gpt_researcher/llm_provider/generic/base.py",
        "lineNo": 7,
        "endLineNo": 124,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fllm_provider%2Fgeneric%2Fbase.py%23L7-L124&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Analysis\n\n**[Quick Summary]**  This Python function acts as a wrapper for different large language model (LLM) providers.  It chooses a specific LLM implementation based on the `provider` argument and provides an interface for obtaining chat responses. The function supports various providers like OpenAI, Anthropic, and HuggingFace, allowing flexibility in model selection.\n\n**[Inputs]**\n* `provider`: A string specifying the LLM provider (e.g., \"openai\", \"anthropic\").\n* `**kwargs`: Keyword arguments passed to the selected LLM provider's constructor. These arguments likely configure the specific model, API key, or other settings.\n\n**[Output]**\n* The function returns a `Chat` object instance with the selected LLM configured. \n\n\n"
    },
    "gpt_researcher__retrievers__exa__exa__ExaSearch": {
        "label": "ExaSearch",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/exa/exa.py",
        "relativePath": "gpt_researcher/retrievers/exa/exa.py",
        "lineNo": 5,
        "endLineNo": 99,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fexa%2Fexa.py%23L5-L99&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Exa API Retriever Analysis\n\n**Quick Summary:**\n\nThis Python code defines a class `ExaSearch` that facilitates interactions with the Exa API for semantic search and document retrieval. It allows users to query Exa, find similar documents, and retrieve document content using a simple API interface. \n\n**Inputs:**\n\n* `query`: The search query to be executed against Exa.\n* `max_results`: The maximum number of search results to return (default 10).\n* `use_autoprompt`:  Flag to indicate whether to use Exa's autoprompt feature (default False).\n* `search_type`: Type of search to perform (e.g., \"neural\", \"keyword\").\n* `**filters`: Additional filters for refining the search results.\n* `url`: The URL of a document for finding similar documents.\n* `exclude_source_domain`: Flag to exclude the source domain from similar document results.\n* `ids`: A list of document IDs for retrieving content.\n* `**options`: Additional options for content retrieval.\n\n**Output:**\n\n* A list of search results, each containing a URL (`href`) and the document text (`body`).\n* A list of similar documents, each with a URL (`href`) and the document text (`body`).\n* A list of document contents, each containing an ID (`id`) and the document text (`content`). \n\n\n"
    },
    "gpt_researcher__retrievers__tavily__tavily_search__TavilySearch": {
        "label": "TavilySearch",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/tavily/tavily_search.py",
        "relativePath": "gpt_researcher/retrievers/tavily/tavily_search.py",
        "lineNo": 10,
        "endLineNo": 99,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Ftavily%2Ftavily_search.py%23L10-L99&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Tavily API Retriever Analysis\n\n**Quick summary:** \n\nThis Python code defines a class `TavilySearch` that interacts with the Tavily API to search for information based on a given query. It handles API key retrieval, constructs search requests, and processes the API response to return a list of relevant sources. The purpose is to provide a convenient way to access and retrieve information from the Tavily search engine programmatically.\n\n**Inputs:**\n\n* `query`: The search query string.\n* `headers`: An optional dictionary of HTTP headers.\n* `topic`: The search topic (e.g., \"general\").\n* `max_results`:  The maximum number of search results to return.\n\n**Output:**\n\n* A list of dictionaries, where each dictionary represents a search result containing:\n    * `href`: The URL of the source.\n    * `body`: The content of the source. \n\n\n\nLet me know if you have any more questions!"
    },
    "gpt_researcher__llm_provider__generic__base__GenericLLMProvider__from_provider": {
        "label": "from_provider",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/llm_provider/generic/base.py",
        "relativePath": "gpt_researcher/llm_provider/generic/base.py",
        "lineNo": 13,
        "endLineNo": 93,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fllm_provider%2Fgeneric%2Fbase.py%23L13-L93&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Function Analysis\n\n**Quick Summary:** This function dynamically instantiates and returns a Large Language Model (LLM) object based on the provided `provider` string. It aims to abstract away the complexities of interacting with different LLM APIs.\n\n**Inputs:**\n\n* `provider`: A string specifying the desired LLM provider (e.g., \"openai\", \"google_genai\", \"huggingface\").\n* `kwargs`: A dictionary containing any provider-specific configuration parameters.\n\n\n**Output:**\n\n* An instance of a LangChain LLM class tailored to the selected provider. \n\n\n\n"
    },
    "gpt_researcher__retrievers__bing__bing__BingSearch": {
        "label": "BingSearch",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/bing/bing.py",
        "relativePath": "gpt_researcher/retrievers/bing/bing.py",
        "lineNo": 9,
        "endLineNo": 88,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fbing%2Fbing.py%23L9-L88&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Bing Search Retriever Code Analysis\n\n**Quick Summary**\n\nThis Python function efficiently searches Bing using a provided API key and returns a list of search results. It utilizes the Bing search API to retrieve relevant webpages based on a user-defined query, normalizes the results for consistency, and excludes YouTube links. The purpose is to provide programmatic access to Bing's search functionality.\n\n**Inputs**\n\n* `query`: User's search query string.\n* `max_results`: (Optional) Maximum number of search results to return (default: 7).\n\n**Output**\n\n* A list of dictionaries, where each dictionary represents a search result containing:\n    * `title`: Title of the webpage.\n    * `href`: URL of the webpage.\n    * `body`: Snippet/summary of the webpage content. \n"
    },
    "gpt_researcher__retrievers__google__google__GoogleSearch": {
        "label": "GoogleSearch",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/google/google.py",
        "relativePath": "gpt_researcher/retrievers/google/google.py",
        "lineNo": 9,
        "endLineNo": 87,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fgoogle%2Fgoogle.py%23L9-L87&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Tavily API Retriever Analysis\n\n**[Quick Summary]**\n\nThis Python function, `search`, is designed to retrieve search results from a Google Custom Search Engine API. It takes a search query as input, utilizes environment variables or provided headers for API and CX keys, and returns a list of search result dictionaries.\n\n**[Inputs]**\n\n*  `query`: The search term to be queried against the Google Custom Search Engine.\n*  `headers`: (Optional) A dictionary containing API keys (e.g., \"google_api_key\" ) and CX keys (\"google_cx_key\") for authentication.\n*  `max_results`: (Optional) The maximum number of search results to return. Defaults to 7.\n\n**[Output]**\n\n\n*  A list of dictionaries, each representing a search result. \n*  Each dictionary contains:\n    *  `title`: The title of the search result.\n    *  `href`: The URL of the search result.\n    *  `body`: A brief snippet of the search result content.\n\n\n\n"
    },
    "gpt_researcher__retrievers__serper__serper__SerperSearch": {
        "label": "SerperSearch",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/serper/serper.py",
        "relativePath": "gpt_researcher/retrievers/serper/serper.py",
        "lineNo": 9,
        "endLineNo": 81,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fserper%2Fserper.py%23L9-L81&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Google Serp Retriever Analysis\n\n**Quick Summary:** This function searches the web using the Serp API and returns a list of top search results formatted as dictionaries. It allows you to programmatically access Google search results and process them.  \n\n**Inputs:**\n\n* `query`: The search query string entered by the user.\n* `max_results`:  The maximum number of search results to be returned (default is 7).\n\n**Output:**\n\n* A list of dictionaries, each representing a search result. \n    * Each dictionary contains \"title\", \"href\" (URL), and \"body\" (snippet) keys. \n\n\n\n"
    },
    "gpt_researcher__master__agent__GPTResearcher____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 20,
        "endLineNo": 91,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L20-L91&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## GPT Researcher Initialization\n\n**Quick Summary:** This Python function initializes a class named `GPT Researcher`. This class likely aims to perform research tasks using a large language model (likely GPT),  constructing a report based on a given query and optional parameters. \n\n**Inputs:**\n\n* **query:** The research question or topic.\n* **report_type:**  Type of report to generate (e.g., \"ResearchReport\").\n* **report_source:** Source of information for the report (e.g., \"Web\").\n* **tone:** Desired writing style for the report (e.g., \"Objective\").\n* **source_urls:** A list of URLs to use as sources.\n* **documents:**  A list of documents to use as sources.\n* **config_path:** Path to a configuration file.\n* **websocket:**  A websocket connection.\n* **agent:** A role or identity for the researcher.\n* **role:**  A specific role or persona for the researcher.\n* **parent_query:**  Main query for detailed reports.\n* **subtopics:** Subtopics related to the main query. \n* **visited_urls:** Set of already visited URLs.\n* **verbose:**  Controls the amount of output.\n* **context:**  Previous interactions or information.\n* **headers:** Dictionary of HTTP headers.\n\n**Output:**\n\n* Initializes the `GPT Researcher` object, ready for research tasks. \n*  The object will have attributes storing the input parameters and other relevant information for the research process.\n\n\n\n"
    },
    "gpt_researcher__master__agent__GPTResearcher__conduct_research": {
        "label": "conduct_research",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 92,
        "endLineNo": 159,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L92-L159&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Breakdown\n\n**Quick Summary:**\n\nThis function, likely part of a research assistant tool, initiates a research process driven by a given query. It utilizes an AI agent to explore the web (or local documents based on configured sources), gathers relevant information, and presents a synthesized context encompassing the research findings. \n\n**Inputs:**\n\n* `query`: The research question or topic.\n* `cfg`: Configuration settings potentially defining search parameters, cost models, etc.\n* `parent_query`:  A potentially related, overarching query.\n* `cost_callback`: A function to track and accumulate research costs.\n* `headers`: HTTP headers for network requests.\n* `verbose`:  A boolean flag controlling output verbosity.\n* `websocket`:  A websocket connection for communication.\n* `source_urls`: (Optional) A list of URLs to use as context for the research.\n* `documents`: (Optional) A list of documents for local context.\n* `report_source`:  A constant specifying the data source (web, local, hybrid, etc.)\n\n**Output:**\n\n* `context`: A string summarizing the research findings, potentially incorporating information from multiple sources. \n"
    },
    "gpt_researcher__retrievers__serpapi__serpapi__SerpApiSearch": {
        "label": "SerpApiSearch",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/serpapi/serpapi.py",
        "relativePath": "gpt_researcher/retrievers/serpapi/serpapi.py",
        "lineNo": 9,
        "endLineNo": 76,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fserpapi%2Fserpapi.py%23L9-L76&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Here's a breakdown of the Python function:\n\n**Quick Summary**\n\nThis function, named `search`, uses the SerpApi library to perform a search query and return a list of search results. It retrieves relevant information like titles, links, and snippets from the search engine results pages (SERPs). The function aims to retrieve a specified number (default 7) of organic search results, excluding YouTube links.\n\n**Inputs**\n\n*  `self.query`: The search term that the user wants to query. \n*  `max_results` (optional): The maximum number of search results to return (defaults to 7).\n\n**Output**\n\n* A list of dictionaries, where each dictionary represents a search result and includes:\n    * `\"title\"`: The title of the web page.\n    * `\"href\"`: The URL of the web page.\n    * `\"body\"`: A brief description or snippet of the web page content. \n\n\nLet me know if you have any other questions! \n"
    },
    "gpt_researcher__master__actions__get_retriever": {
        "label": "get_retriever",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 15,
        "endLineNo": 80,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L15-L80&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function acts as a retriever factory, dynamically loading and returning the appropriate retriever class based on the provided `retriever` name. Its purpose is to provide a flexible way to choose from various search engine and knowledge source retrievers.\n\n## Inputs\n\n* `retriever`: A string value representing the desired retriever name (e.g., \"google\", \"searx\", \"arxiv\").\n\n## Output\n\n*  A Retriever class instance corresponding to the specified `retriever` name.\n*  `None` if an invalid retriever name is provided. \n\n\n"
    },
    "gpt_researcher__master__actions__generate_report": {
        "label": "generate_report",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 391,
        "endLineNo": 453,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L391-L453&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "[Summary]\nThe `generate_report` function constructs and sends a prompt to a large language model (LLM) to generate a report. It tailors the prompt based on the desired report type (`subtopic_report` or another type), incorporates user-provided context, and controls aspects like tone, formatting, and word limit.\n\n[Inputs]\n* `query`:  The initial question or topic for the report.\n* `context`: Additional background information relevant to the query.\n* `agent_role_prompt`:  Instructions setting the LLM's persona or role for report generation.\n* `report_type`: Specifies the kind of report to create (e.g., \"subtopic_report\").\n* `tone`: Desired style of the report (e.g., formal, informal, neutral).\n* `report_source`: Source material or information to draw upon for the report.\n* `websocket`:  Likely a connection for real-time communication with the LLM.\n* `cfg`: Configuration parameters containing settings for the LLM and report generation.\n* `main_topic`: A central theme or subject for the report.\n* `existing_headers`:  Predefined headers for the report structure.\n* `relevant_written_contents`:  Text sections already generated that should be included.\n* `cost_callback`:  A function to track and estimate LLM usage costs.\n* `headers`:  Possibly containing API keys or other authentication information.\n\n[Output]\n* `report`:  The generated text report created by the LLM. \n\n\n\n"
    },
    "gpt_researcher__master__agent__GPTResearcher__write_report": {
        "label": "write_report",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 160,
        "endLineNo": 221,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L160-L221&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Analysis \n\n**Quick Summary:**  This function generates a report based on user-defined parameters like the research query, context, desired report type, tone, and optional additional information. It dynamically chooses how to generate the report based on the `report_type` being either \"custom_report\", \"subtopic_report\", or a default type.\n\n**Inputs:**\n\n*  `self.query`: The research question or topic.\n*  `self.context`:  Background information or previous interactions related to the query.\n*  `self.report_type`:  Specifies the format of the desired report (\"custom_report\", \"subtopic_report\", or default).\n*  `self.report_source`:  Where the report content should come from (e.g., web, database).\n*  `self.role`: Role the system should adopt while generating the report (e.g., \"summarizer\", \"analyst\").\n*  `self.tone`: Desired writing style or mood of the report (e.g., \"formal\", \"informal\").\n*  `self.websocket`:  Connection for real-time communication (likely for progress updates).\n*  `self.cfg`:  Configuration object containing settings and parameters.\n*  `self.headers`:  Possibly a list of predefined headers for the report.\n*  `existing_headers` (for \"subtopic_report\"):  Existing headers from a parent report.\n*  `relevant_written_contents` (for \"subtopic_report\"):  Previously generated text related to the subtopic.\n\n**Output:**\n\n*   `report`: A string containing the generated report. \n"
    },
    "gpt_researcher__master__actions__summarize": {
        "label": "summarize",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 266,
        "endLineNo": 324,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L266-L324&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary \n\nThis asynchronous Python function summarizes a list of URLs provided as input. It breaks down the raw content of each URL into manageable chunks and uses an agent (likely a large language model) to generate a summary for each chunk. Finally, it combines the individual summaries into a single coherent summary for each URL.\n\n## Inputs\n\n* `query`:  The search query associated with the URLs.\n* `content`: A list of dictionaries, each containing an 'url' and 'raw_content' key.\n* `agent_role_prompt`: A prompt defining the role of the agent (likely a language model) used for summarization.\n* `cfg`: Configuration object potentially containing parameters for the summarization process.\n* `websocket`: An optional websocket object for streaming logs or updates.\n* `cost_callback`:  An optional callback function to handle cost estimation during summarization.\n\n\n\n## Output\n\n* A list of dictionaries, each containing:\n    * `url`:  The original URL.\n    * `summary`: The generated summary of the URL's content.  \n"
    },
    "gpt_researcher__retrievers__bing__bing__BingSearch__search": {
        "label": "search",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/bing/bing.py",
        "relativePath": "gpt_researcher/retrievers/bing/bing.py",
        "lineNo": 34,
        "endLineNo": 88,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fbing%2Fbing.py%23L34-L88&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Bing Search API Function Analysis\n\n**Quick Summary:** This function searches the internet using the Bing Search API. It takes a search query, a maximum number of results, and a Bing API key as input. It then returns a list of search results, each containing a title, URL, and snippet of the result page.\n\n**Inputs:**\n\n*  `self.query`: The search query string.\n*  `max_results`: The maximum number of search results to return.\n*  `self.api_key`: A unique API key provided by Bing for authentication.\n\n**Output:**\n\n* A list of dictionaries, each representing a search result.\n* Each dictionary contains:\n    * `title`: The title of the search result.\n    * `href`: The URL of the search result.\n    * `body`: A short snippet of text from the search result page. \n\n\n"
    },
    "gpt_researcher__retrievers__semantic_scholar__semantic_scholar__SemanticScholarSearch": {
        "label": "SemanticScholarSearch",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/semantic_scholar/semantic_scholar.py",
        "relativePath": "gpt_researcher/retrievers/semantic_scholar/semantic_scholar.py",
        "lineNo": 6,
        "endLineNo": 59,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fsemantic_scholar%2Fsemantic_scholar.py%23L6-L59&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Okay, here's a breakdown of the code snippet.\n\n**[Quick Summary]** \n\nThis function searches Semantic Scholar for research papers matching a given query. It sorts the results based on the specified criterion (relevance, citation count, or publication date) and returns a list of paper titles, URLs to open-access PDFs (if available), and abstracts.\n\n**Purpose:** The purpose of this code is to provide a simple way to retrieve research paper information from Semantic Scholar programmatically. \n\n**[Inputs]**\n* `query`:  The search term or phrase to use in the Semantic Scholar API query.\n* `sort`: The criterion for sorting the search results (\"relevance,\" \"citationCount,\" or \"publicationDate\"). Defaults to \"relevance.\"\n* `max_results`: The maximum number of search results to return. Defaults to 20.\n\n**[Output]** \n* A list of dictionaries, where each dictionary represents a search result and contains:\n    * `title`:  The title of the paper.\n    * `href`: A URL to the open-access PDF of the paper (if available). A \"No URL\" string is returned if no open-access PDF is found.\n    * `body`: The abstract of the paper. A \"Abstract not available\" string is returned if no abstract is found. \n\n\n\nLet me know if you'd like me to elaborate on any of these points!\n"
    },
    "gpt_researcher__utils__llm__create_chat_completion": {
        "label": "create_chat_completion",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/utils/llm.py",
        "relativePath": "gpt_researcher/utils/llm.py",
        "lineNo": 22,
        "endLineNo": 74,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Futils%2Fllm.py%23L22-L74&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function uses a specified LLM (likely an AI language model) to generate a text response based on a list of input messages. The code aims to interact with an LLM API,  format the user's input, send it to the API, and return the generated response as text.\n\n## Inputs\n\n- `messages`: A list of dictionaries, each representing a message with properties like \"role\" (likely \"user\" or \"assistant\") and \"content\".\n- `model`: The name of the specific AI model to be used (e.g., \"gpt-3.5-turbo\").\n- `temperature`: A float value controlling the randomness of the generated text (higher = more random).\n- `max_tokens`: An integer limiting the length of the generated response (in tokens).\n- `llm_provider`:  The name of the service providing the LLM (e.g., \"openai\").\n- `openai_api_key`:  The API key for interacting with the OpenAI service.\n- `stream`: A boolean flag indicating whether the response should be streamed (sent in parts).\n- `websocket`: A WebSocket object potentially used for real-time communication with the LLM.\n- `llm_kwargs`: Additional keyword arguments specific to the chosen LLM provider.\n- `cost_callback`: A function to be called to update cost estimates during the API interaction.\n\n## Output\n\n- `str`: A string containing the generated text response from the LLM. \n\n\n"
    },
    "gpt_researcher__document__document__DocumentLoader": {
        "label": "DocumentLoader",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/document/document.py",
        "relativePath": "gpt_researcher/document/document.py",
        "lineNo": 15,
        "endLineNo": 66,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fdocument%2Fdocument.py%23L15-L66&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary:\n\nThis Python function initializes a document loader and asynchronously loads content from multiple files within a given directory. It supports various file formats like PDF, TXT, DOC, etc., extracts text content, and returns a list of dictionaries, each containing the raw text and original file URL. \n\n## Inputs:\n\n*  `path`: A string representing the absolute or relative path to the directory containing the files to be loaded.\n\n\n## Output:\n\n*  A list of dictionaries, where each dictionary contains:\n    * `raw_content`: The extracted text content from the file.\n    * `url`: The original file URL (basename of the file path). \n"
    },
    "gpt_researcher__master__actions__get_sub_queries": {
        "label": "get_sub_queries",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 190,
        "endLineNo": 239,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L190-L239&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Analysis\n\n**[Quick Summary]**\n\nThis function generates subqueries from a given user query. It utilizes a large language model (LLM) to decompose the original query into smaller, more manageable search components. This is likely part of a larger search or information retrieval system designed to improve query understanding and efficiency.\n\n**[Inputs]**\n\n* `query`: The original user's search query.\n* `agent_role_prompt`: Instructions defining the role of the LLM in processing the query.\n* `cfg`: A configuration object likely containing parameters for LLM interaction (e.g., model, temperature).\n* `parent_query`: An overarching query context, potentially for hierarchical searches.\n* `report_type`:  Specifies the desired format or type of information to be extracted.\n* `cost_callback`: A function to handle cost monitoring during LLM interaction.\n* `openai_api_key`:  API key for interacting with an OpenAI LLM.\n\n**[Output]**\n\n* `sub_queries`:  A list of extracted subqueries derived from the original input query. \n\n\n\n"
    },
    "gpt_researcher__scraper__beautiful_soup__beautiful_sou__BeautifulSoupScraper": {
        "label": "BeautifulSoupScraper",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/beautiful_soup/beautiful_soup.py",
        "relativePath": "gpt_researcher/scraper/beautiful_soup/beautiful_soup.py",
        "lineNo": 4,
        "endLineNo": 53,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Fbeautiful_soup%2Fbeautiful_soup.py%23L4-L53&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThe code defines a class that scrapes content from a given URL. It fetches the webpage, removes script and style tags, extracts text content from specific HTML tags (`p`, `h1` to `h5`), and returns the cleaned text. The purpose is to extract and process textual information from a webpage.\n\n## Inputs\n\n* `link`: A string representing the URL of the webpage to scrape. \n* `session`: (optional) An existing web session object. \n\n## Output\n\n* A string containing the cleaned and extracted text content from the webpage. \n"
    },
    "gpt_researcher__retrievers__custom__custom__CustomRetriever": {
        "label": "CustomRetriever",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/custom/custom.py",
        "relativePath": "gpt_researcher/retrievers/custom/custom.py",
        "lineNo": 6,
        "endLineNo": 52,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fcustom%2Fcustom.py%23L6-L52&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Quick Summary\n\nThis Python function defines a custom API retriever that searches a specified endpoint using a provided query. It retrieves results in JSON format and handles potential errors during the API call.\n\nThe purpose is to interface with a backend API for retrieving search results and handle sensitive API parameters securely through environment variables. \n\n## Inputs\n\n* `query`: A string representing the search query to be sent to the API.\n\n* `max_results`: An integer representing the maximum number of results to return (currently unused).\n\n* Environment variables prefixed with `RETRIEVER_ARG_`: These hold various parameters required by the  API endpoint.\n\n## Output\n\n*  A list of dictionaries, each dictionary representing a search result with \"url\" and \"raw_content\" keys.\n*  `None` if an error occurs during the API call. \n\n\n\n"
    },
    "gpt_researcher__retrievers__serper__serper__SerperSearch__search": {
        "label": "search",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/serper/serper.py",
        "relativePath": "gpt_researcher/retrievers/serper/serper.py",
        "lineNo": 35,
        "endLineNo": 81,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fserper%2Fserper.py%23L35-L81&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis\n\n**[Quick Summary]**\nThe function simulates a Google search using the Serp API.  It takes a search query and an optional maximum number of results, retrieves the top organic search results from Google, and returns them in a normalized format. \n\n**[Inputs]**\n*  `self.query`: The search query string.\n*  `max_results`: An integer representing the maximum number of search results to retrieve (default likely 10).\n*  `self.api_key`: A unique API key for accessing the Serp API.\n\n**[Output]**\n*  `search_results`: A list of dictionaries, each representing a search result. \n    *  Each dictionary contains:\n        *  `title`: The title of the search result.\n        *  `href`: The URL of the search result.\n        *  `body`:  A short description (snippet) of the search result. \n\n\n\n"
    },
    "gpt_researcher__master__agent__GPTResearcher____scrape_data_by_query": {
        "label": "__scrape_data_by_query",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 338,
        "endLineNo": 383,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L338-L383&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis: \n\n**Quick summary:** This function performs a multi-source search using various retrievers. It gathers unique URLs from the combined search results and then scrapes the content from those URLs. The purpose is to efficiently collect diverse information from multiple sources based on a given sub-query.  \n\n**Inputs:**\n\n* `sub_query (str)`: The search term or query.\n* `self.retrievers`: A collection of retriever classes, each representing a different search engine or data source.\n* `self.cfg`: Configuration settings, likely including parameters like `max_search_results_per_query`.\n* `self.verbose`: A boolean indicating whether to log the research process. \n* `self.websocket`: A websocket object, probably used for communication  \n\n**Output:**\n\n* `scraped_content_results`: A list containing the scraped content from the retrieved URLs. \n\n\n\n"
    },
    "gpt_researcher__retrievers__pubmed_central__pubmed_central__PubMedCentralSearch__search": {
        "label": "search",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/pubmed_central/pubmed_central.py",
        "relativePath": "gpt_researcher/retrievers/pubmed_central/pubmed_central.py",
        "lineNo": 38,
        "endLineNo": 82,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fpubmed_central%2Fpubmed_central.py%23L38-L82&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "**[Quick Summary]**\n\nThis function performs a search on PubMed Central for research articles containing the full text and returns a list of search results. It leverages the PubMed Central API to retrieve article IDs, downloads the full XML content of selected articles, and extracts relevant information like title, abstract, and a snippet of the article body.\n\n**[Inputs]**\n\n- `max_results`:  The maximum number of search results to retrieve.\n\n**[Output]**\n\n- `search_response`: A list of dictionaries, each containing:\n    - `href`:  A URL link to the NCBI article page.\n    - `body`: A string containing the article title, abstract, and a truncated excerpt of the article body. \n\n\n"
    },
    "gpt_researcher__context__compression__ContextCompressor": {
        "label": "ContextCompressor",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/compression.py",
        "relativePath": "gpt_researcher/context/compression.py",
        "lineNo": 16,
        "endLineNo": 59,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fcompression.py%23L16-L59&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis Python code defines a class designed to retrieve relevant documents from a collection based on a user query. It utilizes contextual compression and embedding techniques to efficiently search and present matching documents. The goal is to provide a fast and focused method for finding relevant information within a large dataset.\n\n\n## Inputs\n\n* **`documents`**: A collection of documents, likely each containing metadata (e.g., source, title) and content.\n* **`embeddings`**: Embeddings generated for each document, enabling semantic similarity comparisons.\n* **`max_results`**: The maximum number of relevant documents to return.\n* **`query`**: The user's search query.\n* **`cost_callback`**:  An optional function to estimate the cost (e.g., computational) of embedding generation.\n\n## Output\n\n* **Beautifully formatted string**: A concisely formatted string displaying the source, title, and content of the top `max_results` relevant documents.  \n\n\n\n***"
    },
    "gpt_researcher__retrievers__serpapi__serpapi__SerpApiSearch__search": {
        "label": "search",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/serpapi/serpapi.py",
        "relativePath": "gpt_researcher/retrievers/serpapi/serpapi.py",
        "lineNo": 35,
        "endLineNo": 76,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fserpapi%2Fserpapi.py%23L35-L76&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis\n\n**Quick Summary:** This function uses the SerpApi to perform a Google search based on a given query. It retrieves a limited number of organic search results (excluding YouTube) and returns a list of dictionaries containing title, URL, and snippet information for each result. \n\n**Inputs:**\n\n* `self.query`: The search query string.\n* `self.api_key`: A unique API key for accessing SerpApi.\n\n**Output:**\n\n* `search_response`: A list of dictionaries, each representing a search result with the following information:\n    * \"title\": The title of the search result.\n    * \"href\": The URL of the search result.\n    * \"body\":  A brief description (snippet) of the search result. \n\n\n\n\n\n"
    },
    "gpt_researcher__master__actions__choose_agent": {
        "label": "choose_agent",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 119,
        "endLineNo": 159,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L119-L159&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function intelligently selects an appropriate agent (a type of AI assistant) based on a given query. It uses a smart LLM to analyze the query and determine the best agent for the task, considering context from a potential parent query. \n\n## Inputs\n\n* **parent_query:**  A related, broader query providing context to the main query.\n* **query:** The main user query requesting information or action.\n* **cfg:** A configuration object containing settings like the LLM model to use and API keys.\n* **cost_callback:**  A function to calculate the cost of using the LLM.\n* **headers:** Possibly containing API keys or other authentication information.\n\n## Output\n\n* **agent:** The name of the selected agent.\n* **agent_role_prompt:** A specific prompt tailored to the chosen agent's role and the query content. \n\n\n"
    },
    "gpt_researcher__master__agent__GPTResearcher____get_context_by_search": {
        "label": "__get_context_by_search",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 238,
        "endLineNo": 278,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L238-L278&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Let's break down this Python code snippet. \n\n**Quick Summary**\n\nThis function retrieves relevant context for a given research query by breaking it down into sub-queries and then scraping information from the web based on these sub-queries.  The goal is to comprehensively gather information related to the original research topic.\n\n**Inputs**\n\n* `query`: The initial research question or topic.\n* `agent_role_prompt`:  A description of the role the code (likely an AI assistant) should play in the research.\n* `cfg`:  Likely a configuration object containing parameters that influence the research process (e.g., number of results to return, sources to search).\n* `parent_query`: Possibly the broader context or topic from which this sub-query originates. \n* `report_type`: Indicates the type of report being generated (e.g., \"subtopic_report\").\n* `cost_callback`: A function used to track and potentially manage the computational cost of the research.\n* `openai_api_key`: An API key for interacting with the OpenAI platform, likely used for language model generation or API calls.\n\n**Output**\n\n* `context`: A list containing the gathered context (likely text snippets, summaries, or links) related to the research queries. \n\n\nLet me know if you have any more questions!\n"
    },
    "gpt_researcher__master__agent__GPTResearcher__get_similar_written_contents_by_draft_section_titles": {
        "label": "get_similar_written_contents_by_draft_section_titles",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 535,
        "endLineNo": 574,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L535-L574&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function retrieves relevant written content based on a subtopic and draft section titles. It searches a database of written contents for matches using multiple queries generated from the provided information.  The results are then filtered and returned as a list of relevant content snippets, prioritizing those matching the provided information most closely.\n\n## Inputs\n\n* `current_subtopic`: The current topic being focused on.\n* `draft_section_titles`:  A list of titles of sections planned or drafted within a larger piece of writing.\n* `written_contents`: A database or list of pre-written content to search.\n* `max_results`:  The maximum number of relevant content snippets to return. Defaults to 10.\n\n\n## Output\n\n* A list of strings: Selected written content snippets deemed relevant to the subtopic and draft section titles.  \n"
    },
    "gpt_researcher__retrievers__pubmed_central__pubmed_central__PubMedCentralSearch__parse_xml": {
        "label": "parse_xml",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/pubmed_central/pubmed_central.py",
        "relativePath": "gpt_researcher/retrievers/pubmed_central/pubmed_central.py",
        "lineNo": 134,
        "endLineNo": 173,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fpubmed_central%2Fpubmed_central.py%23L134-L173&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary \n\nThis function parses XML content representing an article and extracts the title, abstract, and body text. Its purpose is to structure the unstructured XML data into a more usable format for processing or display. \n\n## Inputs\n\n*  `xml_content`: This is the raw XML text containing the article's information.\n\n## Output\n\n*  A dictionary containing three key-value pairs:\n    * `\"title\"`: The article's title as a string.\n    * `\"abstract\"`: The article's abstract as a string.\n    * `\"body\"`: The article's body text as a single string, with paragraphs separated by newline characters. \n\n\n\n"
    },
    "gpt_researcher__retrievers__tavily__tavily_search__TavilySearch___search": {
        "label": "_search",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/tavily/tavily_search.py",
        "relativePath": "gpt_researcher/retrievers/tavily/tavily_search.py",
        "lineNo": 43,
        "endLineNo": 81,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Ftavily%2Ftavily_search.py%23L43-L81&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis \n\n**Quick Summary** \n\nThis function sends a search request to an API endpoint. It constructs a payload with various search parameters, such as query, depth, topic, and filter options, and then sends a POST request to the API. The function returns the API response as JSON if successful or raises an HTTPError if the request fails.\n\n**Inputs**\n\n*  `query`: The search query string.\n*  `search_depth`:  Specifies the level of search detail (\"basic\" or \"advanced\").\n*  `topic`: The area of focus for the search.\n*  `days`: Number of days worth of data to search.\n*  `max_results`: Maximum number of search results to return.\n*  `include_domains`:  A list of permitted domains for results.\n*  `exclude_domains`: A list of domains to exclude from results.\n*  `include_answer`: Flag to include direct answers in the response.\n*   `include_raw_content`: Flag to include raw content (not processed) in the response.\n*   `include_images`: Flag to include images in the results.\n*   `use_cache`: Flag to utilize cached search results.\n*  `api_key`:  An API key for authentication.\n\n**Output**\n\n*  A dictionary containing the JSON response from the API. \n\n\n\n"
    },
    "gpt_researcher__config__config__Config____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/config/config.py",
        "relativePath": "gpt_researcher/config/config.py",
        "lineNo": 9,
        "endLineNo": 46,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fconfig%2Fconfig.py%23L9-L46&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Config Initialization \n\n**Quick Summary:** This code initializes a configuration class for an application, likely a research or information retrieval system. It sets various parameters like data sources, model choices, API keys, and behavior settings based on environment variables and a potential configuration file. \n\n**Inputs:**\n* `config_file`: Path to a configuration file (optional).\n* Environment variables:\n    * `RETRIEVER`: Determines the type of document retriever used.\n    * `EMBEDDING_PROVIDER`: Specifies the provider for generating document embeddings.\n    * `SIMILARITY_THRESHOLD`: Sets the minimum similarity score for retrieving relevant documents.\n    * `LLM_PROVIDER`: Chooses the large language model provider.\n    * `OLLAMA_BASE_URL`:  Base URL for an Ollama API instance.\n    * `FAST_LLM_MODEL`: Name of the fast LLM model.\n    * `SMART_LLM_MODEL`:  Name of the smart LLM model.\n    * `FAST_TOKEN_LIMIT`: Maximum number of tokens for fast LLM interactions.\n    * `SMART_TOKEN_LIMIT`: Maximum number of tokens for smart LLM interactions. \n    * `BROWSE_CHUNK_MAX_LENGTH`: Maximum document chunk length for browsing. \n    * `SUMMARY_TOKEN_LIMIT`: Maximum tokens for generated summaries.\n    * `TEMPERATURE`: Controls the randomness of LLM responses.\n    * `LLM_TEMPERATURE`: Controls the randomness of the LLM. \n    * `USER_AGENT`: User agent string for API requests.\n    * `MAX_SEARCH_RESULTS_PER_QUERY`:  Maximum number of search results returned.\n    * `MEMORY_BACKEND`: Type of memory backend used. \n    * `TOTAL_WORDS`: Total word limit for generated content. \n    * `REPORT_FORMAT`: Desired format for generated reports (e.g., APA).\n    * `MAX_ITERATIONS`: Maximum number of iterations for report generation.\n    * `AGENT_ROLE`: Specifies the role of the agent in the interaction.\n    * `SCRAPER`:  Type of web scraper to use. \n    * `MAX_SUBTOPICS`: Maximum number of subtopics to consider.\n    * `REPORT_SOURCE`:  Source for the report generation.\n    * `DOC_PATH`:  Path to a specific document.\n\n\n\n\n**Outputs:**\n\n* A configured `Config` object with the specified settings."
    },
    "gpt_researcher__context__compression__WrittenContentCompressor": {
        "label": "WrittenContentCompressor",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/compression.py",
        "relativePath": "gpt_researcher/context/compression.py",
        "lineNo": 60,
        "endLineNo": 97,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fcompression.py%23L60-L97&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Analysis\n\n**[Quick Summary]**\n\nThis code defines a class for retrieving relevant documents from a collection based on a given query. It utilizes  text compression and embedding-based similarity to efficiently find the most relevant documents and present them in a formatted way. The purpose is to create a system for semantic document search.\n\n\n**[Inputs]**\n\n*  `documents`: A collection of documents (likely with metadata like titles and content).\n*  `embeddings`:  Pre-computed vector representations (embeddings) of the documents.\n*  `similarity_threshold`: A value determining the minimum similarity required for a document to be considered relevant.\n*  `query`:  The user's search query.\n*  `max_results`: The maximum number of relevant documents to return.\n*  `cost_callback`:  A function to estimate the cost of embedding generation (optional).\n\n**[Output]**\n\n*  A formatted list of the top `max_results` most relevant documents, including their title and content. \n\n\n"
    },
    "gpt_researcher__master__prompts__generate_resource_report_prompt": {
        "label": "generate_resource_report_prompt",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/prompts.py",
        "relativePath": "gpt_researcher/master/prompts.py",
        "lineNo": 90,
        "endLineNo": 127,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fprompts.py%23L90-L127&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Breakdown\n\n**[Quick Summary]** \n\nThis function generates a detailed prompt for an AI assistant to create a resource report, tailoring it to a specific research question and context.  It emphasizes the need for well-structured, informative, and relevant resource recommendations with proper citations. \n\n**[Inputs]**\n\n*   **question (str):** The specific research question the report should address.\n*   **context (str):** A summary of existing research or background information related to the question.\n*   **report_source (str):** Specifies if the sources should be URLs (web resources) or document names.\n*   **report_format (str, optional):**  The desired formatting style (default is APA).\n*   **tone (str, optional):**  The desired tone of the report (e.g., formal, informal).\n*   **total_words (int):** The minimum desired word count for the report.\n\n**[Output]**\n\n*   **str:** A well-formatted prompt instructing the AI assistant to generate a resource report with specific requirements. \n\n\n\n"
    },
    "gpt_researcher__retrievers__searx__searx__SearxSearch": {
        "label": "SearxSearch",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/searx/searx.py",
        "relativePath": "gpt_researcher/retrievers/searx/searx.py",
        "lineNo": 8,
        "endLineNo": 45,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fsearx%2Fsearx.py%23L8-L45&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Tavily API Retriever Analysis\n\n\n**[Quick summary]**\nThis Python code defines a class called `TavilySearch` that searches the web using the Searx metasearch engine. It takes a user query and retrieves up to 7 search results, each containing a link and a snippet of text. \n\n**[Inputs]**\n* `query` (string): The search term the user wants to find.\n* `max_results` (int): The maximum number of results to return (defaults to 7). \n\n**[Output]**\n* `search_response`: A list of dictionaries.\n    * Each dictionary represents a search result.\n    * It contains two keys:\n        * `href`: The URL of the search result.\n        * `body`: A brief summary or snippet of text from the search result. \n\n\n\n"
    },
    "gpt_researcher__scraper__scraper__Scraper__get_scraper": {
        "label": "get_scraper",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/scraper.py",
        "relativePath": "gpt_researcher/scraper/scraper.py",
        "lineNo": 56,
        "endLineNo": 93,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Fscraper.py%23L56-L93&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Quick Summary:\n\nThis function `get_scraper` selects the appropriate scraper class based on the provided link. It aims to dynamically choose the best tool for extracting data from web pages or PDFs depending on their format. \n\n## Inputs:\n\n*  **link:** A URL pointing to a webpage or PDF document.\n\n## Output:\n\n*  **scraper_class:** The selected scraper class (e.g., `PyMuPDFScraper`, `ArxivScraper`) suitable for processing the given link content. \n\n\n"
    },
    "gpt_researcher__retrievers__arxiv__arxiv__ArxivSearch": {
        "label": "ArxivSearch",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/arxiv/arxiv.py",
        "relativePath": "gpt_researcher/retrievers/arxiv/arxiv.py",
        "lineNo": 4,
        "endLineNo": 40,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Farxiv%2Farxiv.py%23L4-L40&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Here's a breakdown of the code snippet:\n\n**Quick Summary**\n\nThis Python function, `__init__` and `search`, acts as a simplified interface to search the arXiv preprint repository.  It takes a search query and optional sorting criteria, then retrieves a specified number of search results, including title, PDF link, and summary. \n\n**Inputs**\n\n* `query`: The text string used to search Arxiv.\n* `sort`: \n    *  'Relevance':  Sorts results based on relevance to the query.\n    *  'SubmittedDate': Sorts results by submission date (newest first).  \n* `max_results`: The maximum number of search results to return. Defaults to 5.  \n\n**Output**\n\n* A list of dictionaries.\n* Each dictionary represents a search result and contains:\n    * `\"title\"`: The title of the preprint.\n    * `\"href\"`: A link to the preprint's PDF.\n    * `\"body\"`: A summary of the preprint. \n\n\n\nLet me know if you have any other questions!"
    },
    "gpt_researcher__master__actions__extract_headers": {
        "label": "extract_headers",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 511,
        "endLineNo": 546,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L511-L546&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis\n\n**[Quick summary]** \nThis function parses Markdown text and extracts all headers, organizing them into a hierarchical structure represented as a list of dictionaries. Each dictionary describes a header with its level and text content, allowing for easy navigation and manipulation of the document's structure.\n\n**[Inputs]**\n\n* `markdown_text`: A string containing Markdown formatted text.\n\n**[Output]**\n\n* `headers`: A list of dictionaries, where each dictionary represents a header.\n\n\n   * Each dictionary contains the following keys:\n      * `level`: An integer representing the header level (e.g., 1 for H1, 2 for H2).\n      * `text`: A string containing the text of the header.   \n\n\n\n\n\n"
    },
    "gpt_researcher__master__actions__summarize_url": {
        "label": "summarize_url",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 325,
        "endLineNo": 360,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L325-L360&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Function Breakdown \n\n**Quick Summary:** This function summarizes text (`raw_data`) based on a user query (`query`). It utilizes a language model (specified by `cfg`) to generate the summary, incorporating an agent role prompt (`agent_role_prompt`) to guide its response. \n\n**Inputs:**\n* `query`: The user's request for a summary.\n* `raw_data`: The text to be summarized.\n* `agent_role_prompt`: A prompt defining the role the language model should assume (e.g., \"summarizer,\" \"text assistant\").\n* `cfg`:  A configuration object likely containing information about the chosen language model, provider, and other parameters.\n* `cost_callback`: A function to track the cost of model usage (optional).\n\n**Output:**\n* `summary`: A string containing the generated text summary. \n\n\n\n"
    },
    "gpt_researcher__master__agent__GPTResearcher____get_similar_written_contents_by_query": {
        "label": "__get_similar_written_contents_by_query",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 499,
        "endLineNo": 534,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L499-L534&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Summary\n\nThis function asynchronously searches a list of previously processed written contents for items similar to a given query. It utilizes pre-computed embeddings to efficiently find relevant matches based on semantic similarity and filters results based on a specified threshold and maximum count. \n\n## Inputs\n\n* **query (str):** The search term or question used to find similar content.\n* **written_contents (List[Dict]):** A pre-processed list of written content, likely containing metadata and embeddings for each item.\n* **similarity_threshold (float, optional):**  The minimum similarity score required for a content item to be considered a match.\n* **max_results (int, optional):** The maximum number of similar contents to return.\n\n## Output\n\n* **List[str]:**  A list of string representations (likely titles or summaries) of the most similar written contents found. \n\n\n"
    },
    "gpt_researcher__master__agent__GPTResearcher____process_sub_query": {
        "label": "__process_sub_query",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 279,
        "endLineNo": 314,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L279-L314&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Function Breakdown \n\n**Quick Summary:**\n\nThis function performs research on a given subquery, scraping website data and extracting relevant context. Its purpose is to gather additional information related to a specific aspect of a larger query.\n\n**Inputs:**\n\n* `sub_query (str)`:  A specific part of a larger query requiring further research. \n* `scraped_data (list)`: Potentially pre-existing scraped data that might be relevant to the subquery.\n\n**Output:**\n\n* `str`: A string containing the gathered context related to the subquery. \n\n\n"
    },
    "gpt_researcher__retrievers__google__google__GoogleSearch__search": {
        "label": "search",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/google/google.py",
        "relativePath": "gpt_researcher/retrievers/google/google.py",
        "lineNo": 52,
        "endLineNo": 87,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fgoogle%2Fgoogle.py%23L52-L87&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Summary\n\nThis function performs a Google Custom Search using a provided API key and search term, returning a list of search results formatted in a standardized way. It excludes YouTube results from the returned list. The purpose of the code is to provide a way to programmatically access Google Search results and integrate them into other applications.\n\n## Inputs\n\n*  `self.query`: The search term to be used in the Google search.\n*  `self.api_key`: A unique API key provided by Google to authenticate the request.\n* `self.cx_key`: A custom search engine ID specific to the user's configuration.\n\n## Output\n\n*  `search_results`: A list of dictionaries.\n*  Each dictionary represents a search result with the following keys:\n    * `title`: The title of the search result.\n    * `href`: The URL link of the search result.\n    * `body`: A short description (snippet) of the search result. \n\n\n"
    },
    "gpt_researcher__retrievers__semantic_scholar__semantic_scholar__SemanticScholarSearch__search": {
        "label": "search",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/semantic_scholar/semantic_scholar.py",
        "relativePath": "gpt_researcher/retrievers/semantic_scholar/semantic_scholar.py",
        "lineNo": 25,
        "endLineNo": 59,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fsemantic_scholar%2Fsemantic_scholar.py%23L25-L59&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary \n\nThis function performs a search on the Semantic Scholar API using a provided query, retrieves up to a specified number of results, and returns a list of dictionaries containing paper titles, download URLs (if open access), and abstracts.  The purpose is to programmatically access and summarize research papers from Semantic Scholar.\n\n## Inputs\n\n*  `max_results`: The maximum number of search results to return.\n* `self.query`: The search query string to be used on Semantic Scholar.\n* `self.sort`: The sorting criteria for search results (likely a string value).\n\n## Output\n\n* A list of dictionaries, where each dictionary represents a research paper and contains the following keys:\n    * `title`: The title of the paper.\n    * `href`: The URL to download the open access PDF of the paper. \n    * `body`: The abstract of the paper. \n\n\n"
    },
    "gpt_researcher__memory__embeddings__Memory": {
        "label": "Memory",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/memory/embeddings.py",
        "relativePath": "gpt_researcher/memory/embeddings.py",
        "lineNo": 5,
        "endLineNo": 37,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmemory%2Fembeddings.py%23L5-L37&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function initializes an embedding provider based on user-specified parameters. Its purpose is to dynamically load and configure different embedding models for use in natural language processing tasks.\n\n## Inputs \n\n*   **embedding_provider**: A string specifying the type of embedding provider to use (e.g., \"ollama\", \"custom\", \"openai\").\n*   **headers**: A dictionary containing optional API keys or configuration details.\n*   **kwargs**: Additional keyword arguments that may be specific to the chosen embedding provider.\n\n## Output\n\n*   **Instance of an embedding provider**: An object representing the selected embedding model, ready to be used for generating embeddings. \n\n\n"
    },
    "gpt_researcher__utils__llm__construct_subtopics": {
        "label": "construct_subtopics",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/utils/llm.py",
        "relativePath": "gpt_researcher/utils/llm.py",
        "lineNo": 75,
        "endLineNo": 107,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Futils%2Fllm.py%23L75-L107&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis \n\n**[Quick Summary]** \n\nThis function generates subtopics from provided data using a large language model (LLM). It leverages a Pydantic validator to structure the LLM's response and a prompt template to guide the generation process. The purpose is to break down textual data into a hierarchical structure of subtopics.\n\n**[Inputs]**\n\n* **task:**  Likely a string describing the specific task or context for subtopic generation.\n* **data:** The main textual input data from which subtopics should be extracted.\n* **subtopics:**  Potentially an initial set of subtopics to guide the process or build upon.\n* **config:** A configuration object containing parameters like LLM model name, temperature, token limit, and other settings.\n* **max_subtopics:** An integer specifying the maximum number of subtopics to generate.\n\n\n**[Output]**\n\n* A structured list of subtopics extracted from the input data. \n"
    },
    "gpt_researcher__context__retriever__SectionRetriever": {
        "label": "SectionRetriever",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/retriever.py",
        "relativePath": "gpt_researcher/context/retriever.py",
        "lineNo": 31,
        "endLineNo": 62,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fretriever.py%23L31-L62&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Summary\n\nThis function, `_get_relevant_documents`, retrieves documents from a list of sections based on a given query using a `CallbackManagerForRetrieverRun` object. The purpose of this code is to implement a simple document retriever that extracts relevant documents from predefined sections.\n\n## Inputs\n\n* `query`: A string representing the search query to find relevant documents.\n\n* `run_manager`:  A `CallbackManagerForRetrieverRun` object likely used for managing the retrieval process and potentially executing callbacks.\n\n## Output\n\n* `docs`: A list of `Document` objects, each containing:\n    *  `page_content`: The text content of the document.\n    * `metadata`: A dictionary including \"section_title\" corresponding to the title of the section the document belongs to. \n\n\n\n"
    },
    "gpt_researcher__master__actions__extract_sections": {
        "label": "extract_sections",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 547,
        "endLineNo": 578,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L547-L578&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Quick Summary\n\nThis function extracts written content from subtopic reports formatted as Markdown. \n\nIt identifies sections based on header tags (e.g., `<h3>`) and isolates the corresponding text content.  \n\nUltimately, it aims to structure the report's information into discrete sections for easier processing or analysis.\n\n## Inputs\n\n* **`markdown_text`**:  The text content of the subtopic report written in Markdown format.\n\n## Outputs\n\n* **`sections`**: A list of dictionaries, where each dictionary represents a section from the report and contains:\n    * `\"section_title\"`: The title of the section extracted from the header tag.\n    * `\"written_content\"`: The text content of the section, cleaned up to remove any HTML tags. \n\n\n"
    },
    "gpt_researcher__master__actions__get_retrievers": {
        "label": "get_retrievers",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 81,
        "endLineNo": 112,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L81-L112&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Breakdown\n\n**[Quick Summary]**\n\nThis function determines which retriever classes to use for searching based on priority: headers, configuration, or a default choice.  Its purpose is to dynamically select the appropriate search method based on provided inputs. \n\n**[Inputs]**\n\n* `headers (dict)`: A dictionary potentially containing information about desired retrievers.\n* `cfg (Config)`: A configuration object likely holding settings for the retriever selection.\n\n**[Output]**\n\n* `list`: A list of retriever classes ready to be used for searching. \n\n\n"
    },
    "gpt_researcher__master__actions__get_report_introduction": {
        "label": "get_report_introduction",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 480,
        "endLineNo": 510,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L480-L510&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Breakdown:\n\n**[Quick summary]** This function generates an introduction for a report based on a user query and provided context. It uses a large language model (LLM) to craft the introduction,  considering the specified role for the response.\n\n**[Inputs]**\n\n*   `query`: Likely a string containing the topic or focus of the report.\n*   `context`: This could be additional text providing background information related to the report.\n*   `role`: A string defining the persona or purpose of the introduction (e.g., \"summarizer\", \"analyst\", \"news reporter\").\n*   `config`: A configuration object holding parameters like the LLM model to use, token limit, and LLM provider.\n*   `websocket`: Potentially a connection for real-time communication with the LLM.\n*   `cost_callback`: A function to track the cost of using the LLM.\n\n\n**[Output]**\n\n*   A string containing the generated report introduction. \n"
    },
    "gpt_researcher__master__agent__GPTResearcher__get_subtopics": {
        "label": "get_subtopics",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 439,
        "endLineNo": 469,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L439-L469&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis async function generates subtopics related to a given task or query. It utilizes pre-existing context, configuration settings, and optionally, user-provided subtopics to construct a list of relevant subtopics.\n\n## Inputs\n\n* `task`: This likely represents the user's initial query or the task to be broken down.\n* `data`:  A dataset or context containing relevant information related to the task.\n* `config`: A configuration object containing parameters that might influence subtopic generation (e.g., complexity, focus areas).\n* `subtopics`:  A list of subtopics provided by the user, which the function might use as a starting point or guide.\n\n## Output\n\n* `subtopics`: A list of generated subtopics relevant to the input task and context. \n"
    },
    "gpt_researcher__master__prompts__generate_search_queries_prompt": {
        "label": "generate_search_queries_prompt",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/prompts.py",
        "relativePath": "gpt_researcher/master/prompts.py",
        "lineNo": 7,
        "endLineNo": 37,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fprompts.py%23L7-L37&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis \n\n**[Quick Summary]** \n\nThis function generates a prompt for a program to create search queries.  Given a question and optionally a parent query, it crafts a prompt instructing a program to generate a list of search queries to find an objective opinion on a given topic.\n\n **[Inputs]**\n\n*  `question`: The specific question to be answered.\n*  `parent_query`: A broader question relevant for more detailed reports.\n*  `report_type`:  Specifies the type of report being generated (e.g., detailed, subtopic).\n*  `max_iterations`:  The maximum number of search queries to generate.  \n\n**[Output]**\n\n* A formatted string containing a prompt for a program to generate search queries. \n    \n    \n \n"
    },
    "gpt_researcher__scraper__web_base_loader__web_base_loader__WebBaseLoaderScraper": {
        "label": "WebBaseLoaderScraper",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/web_base_loader/web_base_loader.py",
        "relativePath": "gpt_researcher/scraper/web_base_loader/web_base_loader.py",
        "lineNo": 2,
        "endLineNo": 32,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Fweb_base_loader%2Fweb_base_loader.py%23L2-L32&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "[Quick summary]\nThis Python function scrapes content from a given webpage URL. It uses the `WebBaseLoader` from the `langchain_community` library to fetch the page and extract its text content. The purpose is to provide a simple way to retrieve and process text data from websites.\n\n[Inputs]\n-  `link`:  The URL of the webpage to scrape.\n- `session`: (Optional) An existing HTTP session object to use for the request. When not provided, a new session will be created.\n\n[Output]\n - A string containing the concatenated text content of the scraped webpage. \n -  If an error occurs during the scraping process, an empty string is returned. \n"
    },
    "gpt_researcher__master__actions__generate_draft_section_titles": {
        "label": "generate_draft_section_titles",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 361,
        "endLineNo": 390,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L361-L390&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Breakdown\n\n**Quick Summary:** This function generates draft section titles for a subtopic report. It does this by using a pre-defined prompt and a fast language model to analyze the provided query, context, and main topic.\n\n**Inputs:**\n\n* `query`: The main query or topic for the report.\n* `context`: Additional information relevant to the report.\n* `agent_role_prompt`:  A prompt defining the role of the AI agent in generating the titles.\n* `report_type`:  Specifies the type of report (\"subtopic_report\" in this case).\n* `websocket`: Likely a WebSocket connection for potential real-time updates or communication.\n* `cfg`:  A configuration object containing model settings and parameters.\n* `main_topic`: The overarching topic of the report.\n* `cost_callback`: A function to track the cost of generating the response.\n* `headers`:  Optional headers for potential API requests.\n\n**Output:**\n\n* `draft_section_titles`: A string containing the generated draft section titles.  \n\n\n\n"
    },
    "gpt_researcher__memory__embeddings__Memory____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/memory/embeddings.py",
        "relativePath": "gpt_researcher/memory/embeddings.py",
        "lineNo": 6,
        "endLineNo": 35,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmemory%2Fembeddings.py%23L6-L35&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "**[Quick summary]** \n\nThis function initializes an embedding provider based on a specified string (`embedding_provider`). It dynamically loads the appropriate embedding library and configuration using environment variables and provided headers. \n\n\n**[Inputs]**\n\n* `embedding_provider`: A string indicating the desired embedding provider (e.g., \"ollama\", \"openai\", \"huggingface\"). \n* `headers`: A dictionary of HTTP headers, potentially containing an `openai_api_key`.\n\n**[Output]**\n\n* An initialized embedding object (`_embeddings`) selected based on the `embedding_provider`. \n\n\n\n"
    },
    "gpt_researcher__scraper__beautiful_soup__beautiful_sou__BeautifulSoupScraper__scrape": {
        "label": "scrape",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/beautiful_soup/beautiful_soup.py",
        "relativePath": "gpt_researcher/scraper/beautiful_soup/beautiful_soup.py",
        "lineNo": 10,
        "endLineNo": 39,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Fbeautiful_soup%2Fbeautiful_soup.py%23L10-L39&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Quick Summary\nThis function extracts clean text content from a webpage by fetching its HTML, removing script and style tags, and then parsing the remaining text. The purpose is to obtain plain textual information from a webpage while excluding potentially irrelevant or disruptive code elements.\n\n## Inputs\n* `self.session.get(self.link, timeout=4)`:  A GET request is made to the URL stored in the `self.link` attribute with a 4-second timeout. \n* `BeautifulSoup(response.content, \"lxml\", from_encoding=response.encoding)`: The HTML content of the response is parsed using BeautifulSoup with the 'lxml' parser, ensuring proper handling of character encoding.\n* `self.get_content_from_url(soup)`: This likely refers to another function within the same class that extracts specific content from the parsed HTML. \n\n\n## Output\n* A string containing the cleaned text content extracted from the webpage. \n* An empty string is returned if an exception occurs during the process. \n"
    },
    "gpt_researcher__master__agent__GPTResearcher__get_draft_section_titles": {
        "label": "get_draft_section_titles",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 470,
        "endLineNo": 498,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L470-L498&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Breakdown\n\n**Quick Summary:** This function dynamically generates draft section titles for a research report based on a given query, context, and specified report type. \n\n**Inputs:**\n\n* `self.query`: The initial research question or topic.\n* `self.context`:  Background information or previous research related to the query.\n* `self.role`:  The desired role or persona for the AI assistant in generating the content. \n* `self.report_type`: The format or style of the report (e.g., summary, analysis, literature review).\n* `self.websocket`: Likely a communication channel for real-time updates or interactions.\n* `self.cfg`:  Configuration settings potentially influencing the generation process.\n* `self.parent_query`: A broader topic potentially providing context for the current query.\n* `self.add_costs`: A function for tracking the computational cost of the process.\n* `self.headers`: Existing headers or structure that might be incorporated.\n\n**Output:**\n\n* A string containing markdown formatted draft section titles. \n\n\n"
    },
    "gpt_researcher__master__actions__table_of_contents": {
        "label": "table_of_contents",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 579,
        "endLineNo": 606,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L579-L606&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Function Breakdown\n\n**Quick Summary:**\n\nThis function takes markdown text as input and generates a structured table of contents (TOC) by extracting header elements and recursively organizing them. The purpose is to automatically create a navigable TOC from markdown documents. \n\n**Inputs:**\n\n* `markdown_text`:  The input markdown document containing header elements.\n* `extract_headers(markdown_text)`: A presumably separate function used to parse the markdown text and identify header elements, likely returning a data structure representing these headers and their hierarchy (e.g., a list of dictionaries).\n\n**Output:**\n\n* A string containing the generated table of contents, formatted with indentation to reflect the hierarchy of headings in the markdown text. \n* If an error occurs during the process, the original markdown text is returned. \n\n\n"
    },
    "gpt_researcher__utils__enum__Tone": {
        "label": "Tone",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/utils/enum.py",
        "relativePath": "gpt_researcher/utils/enum.py",
        "lineNo": 21,
        "endLineNo": 48,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Futils%2Fenum.py%23L21-L48&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Please provide the code selection you'd like me to analyze.  \n\n"
    },
    "gpt_researcher__retrievers__pubmed_central__pubmed_central__PubMedCentralSearch__has_body_content": {
        "label": "has_body_content",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/pubmed_central/pubmed_central.py",
        "relativePath": "gpt_researcher/retrievers/pubmed_central/pubmed_central.py",
        "lineNo": 107,
        "endLineNo": 133,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fpubmed_central%2Fpubmed_central.py%23L107-L133&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function checks if an XML document representing an article contains a body section. It first locates the `<article>` tag within the XML. If a `<body>` tag is found, it returns `True`. Otherwise, it examines each `<sec>` and `<p>` tag to see if any contain text content, indicating a possible body presence. \n\n## Inputs\n\n*   `xml_content`: The XML content of the article being analyzed \n\n\n## Output\n\n*   `True`: If the XML document has a body section, explicitly marked or implicitly through the presence of text content within `<sec>` or `<p>` tags.\n*   `False`: If the XML document lacks a distinct body section and no text content is found within `<sec>` or `<p>` tags. \n"
    },
    "gpt_researcher__master__actions__scrape_urls": {
        "label": "scrape_urls",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 240,
        "endLineNo": 265,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L240-L265&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function `scrape_urls` aims to fetch content from a list of given URLs. It uses a `Scraper` object, potentially configured with a user agent and a specific scraping method, to retrieve the content. The function handles potential errors during scraping.\n\n## Inputs\n\n*  `urls`: A list of web addresses.\n* `cfg`: An optional configuration object that might contain:\n    * `user_agent`: A string representing the user agent to be used for making requests.\n    * `scraper`: An object or identifier specifying the scraping method.\n\n## Output\n\n* `content`: A list of strings, presumably containing the extracted content from each URL."
    },
    "gpt_researcher__master__actions__stream_output": {
        "label": "stream_output",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 454,
        "endLineNo": 479,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L454-L479&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Function Breakdown \n\n**[Quick Summary]** \nThis function streams data in a structured format to a potential websocket connection. If no websocket is provided, it simply prints the data to the console.  This is likely part of a larger system handling communication and output.\n\n**[Inputs]**\n* `type`: A string indicating the type of data being sent.\n*  `content`: Likely a string or other data containing additional context about the output.\n* `output`: The primary data being streamed, possibly a string, list, or dictionary.\n* `websocket`: An optional websocket object to send data through.\n* `logging`: A boolean indicating whether to print output to the console. \n* `metadata`: Optional additional data to be included with the output.\n\n**[Output]**\n*  If `websocket` is provided: A JSON object containing \"type\", \"content\", \"output\", and optionally \"metadata\" is sent over the websocket.\n* If `websocket` is not provided or `logging` is True: The `output` is printed to the console. \n"
    },
    "gpt_researcher__retrievers__arxiv__arxiv__ArxivSearch__search": {
        "label": "search",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/arxiv/arxiv.py",
        "relativePath": "gpt_researcher/retrievers/arxiv/arxiv.py",
        "lineNo": 15,
        "endLineNo": 40,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Farxiv%2Farxiv.py%23L15-L40&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Breakdown\n\n**[Quick Summary]**\nThis function searches the arXiv repository for scientific papers based on a given query. It retrieves a list of matching papers, each containing its title, PDF link, and abstract. The results are returned as a structured list suitable for further processing or display.\n\n**[Inputs]**\n\n* `query`:  The search term or phrase used to query the arXiv database.\n* `max_results`: The maximum number of search results to be returned.\n\n**[Output]**\n\n* A list of dictionaries, where each dictionary represents a search result and contains:\n    * `title`: The title of the scientific paper.\n    * `href`: A URL link to the PDF version of the paper.\n    * `body`: The abstract of the scientific paper. \n\n\n\n\n"
    },
    "gpt_researcher__retrievers__exa__exa__ExaSearch__search": {
        "label": "search",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/exa/exa.py",
        "relativePath": "gpt_researcher/retrievers/exa/exa.py",
        "lineNo": 40,
        "endLineNo": 65,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fexa%2Fexa.py%23L40-L65&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Analysis of Function Code\n\n**Quick Summary:** \n\nThis function searches a query using the Exa API. It retrieves a specified number of results (max_results) based on the selected search type (e.g., \"neural\" or \"keyword\") and applies optional filters.  \n\n **Inputs:**\n\n* `max_results`: Maximum number of search results to return.\n* `use_autoprompt`:  Flag to enable/disable autoprompting.\n* `search_type`: Type of search to perform (\"neural\" or \"keyword\", etc.).\n* `**filters`:  Dictionary of additional filters to refine the search (e.g., date range, domains).\n\n**Output:**\n\n* A list of search results. Each result is a dictionary with:\n    *  `href`: URL of the result.\n    *  `body`: Text content of the result. \n\n\n\n\n"
    },
    "gpt_researcher__scraper__web_base_loader__web_base_loader__WebBaseLoaderScraper__scrape": {
        "label": "scrape",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/web_base_loader/web_base_loader.py",
        "relativePath": "gpt_researcher/scraper/web_base_loader/web_base_loader.py",
        "lineNo": 8,
        "endLineNo": 32,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Fweb_base_loader%2Fweb_base_loader.py%23L8-L32&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Analysis of Python Code Snippet\n\n**Quick Summary:**\n\nThis function scrapes text content from a given webpage using the `WebBaseLoader` from the `langchain_community` library. It combines the content from all loaded documents and returns it as a single string. The purpose is to extract and aggregate text information from a web page. \n\n**Inputs:**\n\n* `self.link`: This variable likely holds a string representing the URL of the webpage to be scraped.\n\n**Output:**\n\n* A string: Containing the concatenated text content of all documents loaded from the webpage. \n* An empty string:  Returned if any exception occurs during the scraping process. \n\n\n"
    },
    "gpt_researcher__document__document__DocumentLoader___load_document": {
        "label": "_load_document",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/document/document.py",
        "relativePath": "gpt_researcher/document/document.py",
        "lineNo": 43,
        "endLineNo": 66,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fdocument%2Fdocument.py%23L43-L66&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function attempts to load various document types (pdf, txt, doc, docx, pptx, csv, xls, xlsx, md) using dedicated loaders. It returns the extracted data from the document or an empty list if loading fails.\n\nThe purpose is to provide a flexible way to handle different document formats and extract their content.\n\n## Inputs\n\n* `file_path`: A string representing the path to the document file.\n\n\n## Output\n* `ret_data`: A list containing the extracted data from the document. An empty list if loading fails. \n"
    },
    "gpt_researcher__retrievers__custom__custom__CustomRetriever__search": {
        "label": "search",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/custom/custom.py",
        "relativePath": "gpt_researcher/retrievers/custom/custom.py",
        "lineNo": 29,
        "endLineNo": 52,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fcustom%2Fcustom.py%23L29-L52&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis Python function performs a web search using a custom API endpoint. It constructs a request URL with the search query and retrieves the JSON response containing search results.\n\n## Inputs\n\n* **`self.endpoint`**: The URL of the custom API endpoint for retrieving search results. \n* **`self.params`**:  A dictionary containing additional parameters to be passed to the API endpoint.\n* **`self.query`**: The search query string to be used in the API request.\n\n## Output\n\n* **JSON Response**: A list of dictionaries, each containing:\n    * `\"url\"`: The URL of the webpage.\n    * `\"raw_content\"`: The raw content retrieved from the webpage. \n    * (*Optional*)  An error message if the API request fails. \n\n\n\n"
    },
    "gpt_researcher__retrievers__pubmed_central__pubmed_central__PubMedCentralSearch__fetch": {
        "label": "fetch",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/pubmed_central/pubmed_central.py",
        "relativePath": "gpt_researcher/retrievers/pubmed_central/pubmed_central.py",
        "lineNo": 83,
        "endLineNo": 106,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fpubmed_central%2Fpubmed_central.py%23L83-L106&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "[Quick summary] This Python function retrieves the full text content of PubMed Central (PMC) articles in XML format.  It uses the NCBI E-utilities API to fetch data based on provided article IDs and an API key. The purpose is to efficiently download article text for further processing or analysis.\n\n[Inputs]\n- `ids`: A list of PubMed Central (PMC) article identifiers (e.g., \"PMC1234567\"). \n- `api_key`: A unique API key used to authenticate access to the NCBI E-utilities API. \n\n[Output]\n-  The full text content of the requested articles in XML format. \n\n\n"
    },
    "gpt_researcher__document__document__DocumentLoader__load": {
        "label": "load",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/document/document.py",
        "relativePath": "gpt_researcher/document/document.py",
        "lineNo": 20,
        "endLineNo": 42,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fdocument%2Fdocument.py%23L20-L42&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis\n\n**Quick Summary:** This function asynchronously iterates through files in a specified directory, extracts document content from each file based on its extension, and generates a list of dictionaries containing the raw content and original file names.\n\nThe purpose is likely to process a directory of documents, potentially for indexing, analysis, or further processing.\n\n\n**Inputs:**\n\n* `self.path`:  A string representing the path to the directory containing the documents.\n\n**Output:**\n\n* A list of dictionaries where each dictionary has two keys:\n    * `raw_content`: The extracted content of the document as a string.\n    * `url`: The original file name of the document. \n"
    },
    "gpt_researcher__master__actions__handle_json_error": {
        "label": "handle_json_error",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 160,
        "endLineNo": 182,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L160-L182&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Analysis:\n\n**[Quick Summary]**\n\nThis function attempts to extract a server name and agent role prompt from a given response string. It first tries to parse the string using `json_repair.loads`, then uses a regular expression to find JSON within the string. If successful, it returns the extracted server and prompt. If no valid JSON is found, it defaults to using a predefined \"Default Agent\" and prompt.\n\n**[Inputs]**\n\n*  **response:** A string containing the response from an external source.\n\n**[Output]**\n\n*  **server:** A string representing the name of the server.\n*  **agent_role_prompt:** A string containing the agent's role description. \n"
    },
    "gpt_researcher__master__agent__GPTResearcher____get_new_urls": {
        "label": "__get_new_urls",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 315,
        "endLineNo": 337,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L315-L337&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Analysis \n\n**Quick Summary:** \n\nThis function identifies and returns new URLs from a provided set that have not been visited before. It updates a `visited_urls` set to keep track of already processed URLs and logs the added URLs to a websocket stream for monitoring.  The purpose is to manage the exploration of URLs within a web crawler or similar application. \n\n\n**Inputs:**\n\n*  `url_set_input`: A set of strings representing URLs to be checked.\n* `self.visited_urls`: A set storing previously visited URLs to avoid redundancy.\n* `self.verbose`:  A boolean flag controlling the level of output logging.\n* `self.websocket`:  A websocket object likely used for communication and feedback.\n\n**Output:**\n\n*  `new_urls`: A list of strings representing the newly identified, unvisited URLs from the input set. \n\n\n"
    },
    "gpt_researcher__master__actions__add_source_urls": {
        "label": "add_source_urls",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 607,
        "endLineNo": 628,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L607-L628&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function takes a Markdown report and a set of visited URLs, then appends a list of referenced URLs to the report in a structured format (as numbered links). This helps organize resources cited within the Markdown document.\n\n## Inputs\n*  `report_markdown`: The original Markdown text of the report.\n*  `visited_urls`: A set containing URLs already present within the `report_markdown`.\n\n## Output\n*  `updated_markdown_report`: The original Markdown report with an appended section of numbered links to the cited URLs. \n"
    },
    "gpt_researcher__context__retriever__SearchAPIRetriever": {
        "label": "SearchAPIRetriever",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/retriever.py",
        "relativePath": "gpt_researcher/context/retriever.py",
        "lineNo": 10,
        "endLineNo": 30,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fretriever.py%23L10-L30&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function, `_get_relevant_documents`, retrieves relevant documents from a collection based on a given query. It assumes the documents are stored within the `self.pages` list, each containing raw content, a title, and a source URL. The function constructs `Document` objects from this data for further processing. \n\n## Inputs\n\n*  `query: str`:  The search query used to filter documents.\n* `run_manager: CallbackManagerForRetrieverRun`: A mechanism likely used for tracking and managing the retrieval process, potentially allowing for callbacks or logging.\n\n## Output\n\n*  `List[Document]`:  A list of `Document` objects, each containing the extracted raw content, title, and source URL. \n"
    },
    "gpt_researcher__llm_provider__generic__base__GenericLLMProvider__stream_response": {
        "label": "stream_response",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/llm_provider/generic/base.py",
        "relativePath": "gpt_researcher/llm_provider/generic/base.py",
        "lineNo": 104,
        "endLineNo": 124,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fllm_provider%2Fgeneric%2Fbase.py%23L104-L124&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "You got it. Here's a breakdown of the code snippet:\n\n**Quick Summary**\n\nThis function streams the output of a large language model (LLM) in a conversational manner, sending responses in paragraphs and periodically updating a WebSocket or printing to the console. It aims to provide a more interactive and real-time experience for interacting with the LLM.\n\n**Inputs**\n\n* `self.llm`: The instance of the large language model \n* `messages`: A sequence of prompts or messages to be sent to the LLM.\n\n**Output**\n\n* `response`: The complete, concatenated text generated by the LLM. \n\n"
    },
    "gpt_researcher__master__agent__GPTResearcher____get_similar_content_by_query": {
        "label": "__get_similar_content_by_query",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 384,
        "endLineNo": 404,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L384-L404&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Breakdown:\n\n**Quick Summary**\n\nThis function retrieves and summarizes relevant content from a set of web pages (\"pages\") based on a user query (\"query\"). It does this by leveraging embeddings and a `ContextCompressor` to efficiently find and condense matching information.\n\n**Inputs**\n\n* `self.verbose`: A boolean indicating whether to display verbose logging messages.\n* `query`: A string representing the user's search query.\n* `pages`: A collection of web pages (documents) to search within.\n* `self.memory`: An object (likely containing pre-computed embeddings) used for efficient content retrieval.\n\n**Output**\n\n* A summarized set of relevant content from the \"pages\" based on the \"query\".\n* The returned content likely includes a maximum of 8 results, ranked by relevance. \n\n\n"
    },
    "gpt_researcher__master__prompts__generate_summary_prompt": {
        "label": "generate_summary_prompt",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/prompts.py",
        "relativePath": "gpt_researcher/master/prompts.py",
        "lineNo": 191,
        "endLineNo": 209,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fprompts.py%23L191-L209&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Okay, here's the breakdown of the provided code snippet.\n\n**[Quick Summary]**\n\nThis function creates a prompt designed to instruct a text summarization AI model. It takes text and a question (or task) as input and constructs a prompt that asks the AI to summarize the text based on that specific query. If the query can't be answered directly from the text, the AI is instructed to provide a general summary.\n\n**[Inputs]**\n\n* **`data` (str):**  The text that needs to be summarized.\n* **`query` (str):**  The question or task that should guide the summarization.\n\n**[Output]**\n\n*  **A string:** \n    *  The formatted prompt, combining the input text and the query, designed for the AI summarizer. \n\n\nLet me know if you have any other code snippets you'd like me to analyze! \n"
    },
    "gpt_researcher__retrievers__exa__exa__ExaSearch__find_similar": {
        "label": "find_similar",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/exa/exa.py",
        "relativePath": "gpt_researcher/retrievers/exa/exa.py",
        "lineNo": 66,
        "endLineNo": 84,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fexa%2Fexa.py%23L66-L84&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary:\n\nThis function uses the Exa API to find web pages similar to a given URL. It returns a list of these similar documents, each containing a link (href) and the document's text (body). The purpose of the code is likely to provide a way for users or applications to discover related content based on a specific URL.\n\n\n## Inputs:\n\n- `url`: The URL of the document for which similar documents are sought.\n- `exclude_source_domain`:  A boolean flag indicating whether to exclude documents from the same domain as the input URL.\n- `**filters`:  Additional keyword arguments that can be passed to refine the search results.  The specific filters and their meaning would depend on the Exa API documentation. \n\n## Output:\n\n- A list of dictionaries, where each dictionary represents a similar document.\n- Each dictionary has two keys:\n    - `href`: The URL of the similar document.\n    - `body`: The text content of the similar document. \n"
    },
    "gpt_researcher__scraper__arxiv__arxiv__ArxivScraper": {
        "label": "ArxivScraper",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/arxiv/arxiv.py",
        "relativePath": "gpt_researcher/scraper/arxiv/arxiv.py",
        "lineNo": 4,
        "endLineNo": 22,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Farxiv%2Farxiv.py%23L4-L22&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Quick Summary\n\nThis function scrapes the content of the first document found on arXiv for a given search query extracted from a provided link. It utilizes the `ArxivRetriever` library to retrieve relevant documents based on the query.  The purpose is to efficiently access and extract information from scientific papers on arXiv. \n\n\n## Inputs\n\n* `link`: A string representing the link to a search query on arXiv (e.g., \"https://arxiv.org/search/?query=machine+learning\").\n* `session`: An optional existing session object, potentially for handling authentication or persistent connections. \n\n\n## Output\n\n*  `page_content`:  A string containing the full text content of the first retrieved document from arXiv matching the query. \n"
    },
    "gpt_researcher__master__prompts__generate_outline_report_prompt": {
        "label": "generate_outline_report_prompt",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/prompts.py",
        "relativePath": "gpt_researcher/master/prompts.py",
        "lineNo": 134,
        "endLineNo": 151,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fprompts.py%23L134-L151&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary:\n\nThis Python function generates a structured outline prompt for a research report based on a provided question and context.  It aims to guide the creation of a well-organized research report with specified word count and formatting.\n\n## Inputs:\n\n*  `question`: The research question or topic.\n* `context`: Background information or a research summary.\n* `report_source`:  \n\nSource of the research report (likely a web URL, dataset, etc.).\n* `report_format`:  Desired report formatting (set as \"apa\" by default).\n* `total_words`: Minimum word count for the final report.\n\n## Output:\n\n* A string containing a detailed markdown syntax formatted outline prompt for the research report. \n\n\n\n"
    },
    "gpt_researcher__retrievers__tavily__tavily_search__TavilySearch__search": {
        "label": "search",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/tavily/tavily_search.py",
        "relativePath": "gpt_researcher/retrievers/tavily/tavily_search.py",
        "lineNo": 82,
        "endLineNo": 99,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Ftavily%2Ftavily_search.py%23L82-L99&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis Python function searches the Tavily API for information related to a given query and topic. It retrieves the URL and content of relevant sources and returns them in a structured format.\n\n## Inputs\n\n*  `self.query`: The search query string.\n*  `max_results`: The maximum number of search results to return.\n\n\n## Output\n\n*  `search_response`: A list of dictionaries, where each dictionary represents a search result containing:\n    *  `href`: The URL of the source.\n    *  `body`: The content of the source. \n"
    },
    "gpt_researcher__scraper__pymupdf__pymupdf__PyMuPDFScraper": {
        "label": "PyMuPDFScraper",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/pymupdf/pymupdf.py",
        "relativePath": "gpt_researcher/scraper/pymupdf/pymupdf.py",
        "lineNo": 4,
        "endLineNo": 21,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Fpymupdf%2Fpymupdf.py%23L4-L21&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThe `scrape` function downloads a PDF document from a given link using PyMuPDFLoader and returns the entire document content as a string.  \n\n## Inputs\n\n* **`link`**: A string representing the URL of the PDF document to be downloaded.\n* **`session`**: A potentially optional object, likely representing a web session, that could be used for handling the download.\n\n## Output\n\n* **String**: A string representation of the entire downloaded PDF document content.  \n\n\n"
    },
    "gpt_researcher__context__retriever__SearchAPIRetriever___get_relevant_documents": {
        "label": "_get_relevant_documents",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/retriever.py",
        "relativePath": "gpt_researcher/context/retriever.py",
        "lineNo": 14,
        "endLineNo": 30,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fretriever.py%23L14-L30&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function processes a list of pages and converts them into a list of `Document` objects. Each `Document` object contains the raw page content and metadata like title and source URL. The purpose of this code is likely to prepare data for indexing or search within a document retrieval system.\n\n\n## Inputs\n\n- `query`: A string representing the user's search query.\n- `run_manager`: A class responsible for managing callbacks during the retrieval process.\n\n## Output\n\n- A list of `Document` objects. \n"
    },
    "gpt_researcher__master__prompts__generate_report_prompt": {
        "label": "generate_report_prompt",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/prompts.py",
        "relativePath": "gpt_researcher/master/prompts.py",
        "lineNo": 38,
        "endLineNo": 54,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fprompts.py%23L38-L54&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Please provide the code selection you would like me to analyze.  \n\n"
    },
    "gpt_researcher__retrievers__exa__exa__ExaSearch___retrieve_api_key": {
        "label": "_retrieve_api_key",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/exa/exa.py",
        "relativePath": "gpt_researcher/retrievers/exa/exa.py",
        "lineNo": 23,
        "endLineNo": 39,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fexa%2Fexa.py%23L23-L39&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\nThis function fetches an Exa API key from an environment variable named \"EXA_API_KEY\". It's intended to securely access the Exa API without hardcoding the key directly in the code. Failing to find the key in the environment variable raises an exception prompting the user to set it. \n\n## Inputs\n- `os.environ[\"EXA_API_KEY\"]`:  A string representing the Exa API key, expected to be set in the `EXA_API_KEY` environment variable.\n\n## Output\n- `api_key`: The retrieved Exa API key as a string. \n\n\n"
    },
    "gpt_researcher__retrievers__pubmed_central__pubmed_central__PubMedCentralSearch___retrieve_api_key": {
        "label": "_retrieve_api_key",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/pubmed_central/pubmed_central.py",
        "relativePath": "gpt_researcher/retrievers/pubmed_central/pubmed_central.py",
        "lineNo": 21,
        "endLineNo": 37,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fpubmed_central%2Fpubmed_central.py%23L21-L37&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Analysis\n\n**Quick Summary:** This function retrieves the NCBI API key from the environment variable `NCBI_API_KEY`. It's purpose is to securely store and access your API key without hardcoding it in the code.\n\n**Inputs:**\n\n*  `os.environ[\"NCBI_API_KEY\"]`: This accesses the value stored in the environment variable named \"NCBI_API_KEY\".\n\n**Output:**\n*  The `api_key` string containing your NCBI API Key. \n\n\n"
    },
    "gpt_researcher__master__agent__GPTResearcher____get_context_by_urls": {
        "label": "__get_context_by_urls",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 222,
        "endLineNo": 237,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L222-L237&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Breakdown: \n\n**Quick Summary:**\n\nThis function scrapes web pages from a given set of URLs, extracts their text content, and then identifies similar content related to a user-provided query. The purpose is to gather information from multiple sources to better understand and respond to a specific user query.\n\n**Inputs:**\n\n*  `urls`: A list of URL addresses.\n*  `self.verbose`: A boolean flag indicating whether verbose logging should be enabled.\n*  `self.websocket`: A websocket connection likely used for communication.\n*  `self.cfg`: Configuration settings for the scraping process.\n*  `self.query`: The user's search query.\n\n**Output:**\n\n*  Relevant content similar to the `self.query` found within the scraped web pages. \n\n\n"
    },
    "gpt_researcher__scraper__scraper__Scraper__extract_data_from_link": {
        "label": "extract_data_from_link",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/scraper.py",
        "relativePath": "gpt_researcher/scraper/scraper.py",
        "lineNo": 40,
        "endLineNo": 55,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Fscraper.py%23L40-L55&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Code Snippet Analysis \n\n**[Quick Summary]** This function aims to extract raw content from a given URL. It uses a scraper object tailored to the specific link and attempts to retrieve the content. If the retrieved content is short (less than 100 characters), it returns a dictionary indicating the URL and that no meaningful content was found. Otherwise, it returns the URL and the extracted raw content.\n\n**[Inputs]**\n* **self.get_scraper(link):** This suggests a method within the class that dynamically returns a suitable scraper object based on the provided link. \n* **link:** The URL from which content needs to be extracted.\n* **session:**  Likely an object representing an established web session, possibly used for handling cookies or other session-related data during the scraping process.\n\n**[Output]**\n*  A dictionary containing:\n    * **url:**  The original URL provided.\n    * **raw_content:** \n       *  The extracted content from the URL if successful and its length is greater than 100 characters.\n       *  `None` if content extraction failed or the retrieved content is less than 100 characters. \n\n\n"
    },
    "gpt_researcher__context__compression__ContextCompressor____get_contextual_retriever": {
        "label": "__get_contextual_retriever",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/compression.py",
        "relativePath": "gpt_researcher/context/compression.py",
        "lineNo": 24,
        "endLineNo": 38,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fcompression.py%23L24-L38&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis \n\n**Quick Summary:** This function creates a `ContextualCompressionRetriever` which is designed to efficiently retrieve relevant documents from a collection (`self.documents`) by compressing and filtering them using embeddings and a recursive text splitting technique. This improves search performance, especially for large document sets.\n\n\n**Inputs:**\n\n* `self.documents`: Presumably a list or iterable of documents to be indexed and searched.\n* `self.embeddings`:  A pre-trained embedding model used to generate vector representations of text. \n* `self.similarity_threshold`: A value used to filter out documents that are dissimilar to a given query.\n\n\n**Output:**\n\n* `contextual_retriever`:  A  `ContextualCompressionRetriever` object ready to be used for efficient document retrieval. \n"
    },
    "gpt_researcher__context__compression__WrittenContentCompressor____get_contextual_retriever": {
        "label": "__get_contextual_retriever",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/compression.py",
        "relativePath": "gpt_researcher/context/compression.py",
        "lineNo": 67,
        "endLineNo": 81,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fcompression.py%23L67-L81&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis\n\n**Quick Summary:** This function builds a `ContextualCompressionRetriever` for efficient document retrieval. It utilizes a pipeline that splits documents, filters relevant sections based on embeddings, and then retrieves those compressed sections in a contextual manner.  This aims to improve retrieval accuracy and efficiency by focusing on relevant information.\n\n**Inputs:**\n\n* `self.embeddings`: A pre-trained embedding model likely used for semantic similarity comparisons.\n* `self.similarity_threshold`: A value determining the minimum similarity required for sections to be considered relevant.\n* `self.documents`: A collection of documents used for retrieval.\n\n**Output:**\n\n* `contextual_retriever`: A `ContextualCompressionRetriever` instance ready to retrieve relevant document sections.  \n\n\n"
    },
    "gpt_researcher__context__retriever__SectionRetriever___get_relevant_documents": {
        "label": "_get_relevant_documents",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/retriever.py",
        "relativePath": "gpt_researcher/context/retriever.py",
        "lineNo": 48,
        "endLineNo": 62,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fretriever.py%23L48-L62&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Code Breakdown \n\n**Quick Summary:**\n\nThis Python function processes a set of webpage sections (`self.sections`) to extract document content and metadata, ultimately returning a list of `Document` objects.  These documents likely represent individual pages or sections from a larger website, designed for querying and retrieval.\n\n**Inputs:**\n\n* `query: str`: A search query string used to filter or match documents.\n* `run_manager: CallbackManagerForRetrieverRun`: An object responsible for managing callbacks during the document retrieval process.\n\n**Output:**\n\n*  `List[Document]`: A list of `Document` objects, each containing page content (`page_content`) and metadata (`section_title`). \n\n\n"
    },
    "gpt_researcher__document__langchain_document__LangChainDocumentLoader": {
        "label": "LangChainDocumentLoader",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/document/langchain_document.py",
        "relativePath": "gpt_researcher/document/langchain_document.py",
        "lineNo": 10,
        "endLineNo": 24,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fdocument%2Flangchain_document.py%23L10-L24&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function processes a list of `Document` objects, extracting the raw page content and a URL from each document's metadata. It returns these as a list of dictionaries, each representing a document with its attributes. The purpose of the code appears to be preparing documents for further processing or analysis, likely by a machine learning model.\n\n## Inputs\n\n*  `documents`: A list of `Document` objects.\n\n## Output\n\n* A list of dictionaries, each containing:\n    * `\"raw_content\"`: The raw text content of a document's page.\n    * `\"url\"`: A string representing the URL of a document, potentially taken from its metadata.  \n\n\n\n"
    },
    "gpt_researcher__retrievers__exa__exa__ExaSearch__get_contents": {
        "label": "get_contents",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/exa/exa.py",
        "relativePath": "gpt_researcher/retrievers/exa/exa.py",
        "lineNo": 85,
        "endLineNo": 99,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fexa%2Fexa.py%23L85-L99&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Summary\n\nThis function retrieves the textual content of documents using the Exa API based on provided IDs. It fetches the documents from the Exa system and returns a structured list containing each document's ID and its corresponding text content.  The purpose of this code is to provide a convenient way to access the textual information stored within Exa documents.\n\n## Inputs\n\n- `ids`: A list of document IDs.\n- `**options`: Additional parameters that can be passed to the Exa API's content retrieval function. These might include options for filtering, sorting, or formatting the retrieved content.\n\n## Output\n\n-  A list of dictionaries.\n- Each dictionary contains two keys: 'id' and 'content'.\n- 'id': The ID of the document.\n- 'content': The textual content of the document. \n\n\n"
    },
    "gpt_researcher__retrievers__tavily__tavily_search__TavilySearch____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/tavily/tavily_search.py",
        "relativePath": "gpt_researcher/retrievers/tavily/tavily_search.py",
        "lineNo": 14,
        "endLineNo": 28,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Ftavily%2Ftavily_search.py%23L14-L28&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  TavilySearch Object Initialization - Code Breakdown\n\n**Quick Summary:**  This code initializes an object called `TavilySearch` designed to interact with the Tavily search API. It sets up the base URL, API key, content type header, and stores the user's query, topic, and any provided custom headers.\n\n**Inputs:**\n\n*  `query`: The search query to be sent to the Tavily API.\n*  `headers`: (Optional) A dictionary of custom headers to be included in the API request.\n \n*  `topic`:  A specific topic associated with the search query.\n\n**Output:**\n\n* Initializes a  `TavilySearch` object ready to send API requests.\n* Stores the user's query, topic, headers, and API key within the object for later use. \n"
    },
    "gpt_researcher__retrievers__google__google__GoogleSearch__get_api_key": {
        "label": "get_api_key",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/google/google.py",
        "relativePath": "gpt_researcher/retrievers/google/google.py",
        "lineNo": 24,
        "endLineNo": 37,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fgoogle%2Fgoogle.py%23L24-L37&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis \n\n**Quick Summary:**\n\nThis function retrieves the Google API key from the `GOOGLE_API_KEY` environment variable. Its purpose is to securely store and access the API key required for interacting with Google services. \n\n**Inputs:**\n\n* **os.environ[\"GOOGLE_API_KEY\"]:** This fetches the value stored in the environment variable named \"GOOGLE_API_KEY\".\n\n\n**Output:**\n\n* **api_key (string):** This will contain the value of the Google API key, which is a unique identifier used to authenticate and authorize access to Google's APIs. \n"
    },
    "gpt_researcher__retrievers__google__google__GoogleSearch__get_cx_key": {
        "label": "get_cx_key",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/google/google.py",
        "relativePath": "gpt_researcher/retrievers/google/google.py",
        "lineNo": 38,
        "endLineNo": 51,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fgoogle%2Fgoogle.py%23L38-L51&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis\n\n**Quick summary:** This function retrieves a Google CX API key from the environment variable `GOOGLE_CX_KEY`. It's designed to handle authentication for interacting with Google Custom Search Engine APIs.\n\n**Inputs:**\n\n* `os.environ[\"GOOGLE_CX_KEY\"]`:  A string containing the API key obtained from Google Cloud Platform.\n\n**Output:**\n\n* The Google CX API key as a string. \n\n\n\nLet me know if you'd like a deeper explanation of any specific part of the code!\n"
    },
    "gpt_researcher__retrievers__searx__searx__SearxSearch__get_api_key": {
        "label": "get_api_key",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/searx/searx.py",
        "relativePath": "gpt_researcher/retrievers/searx/searx.py",
        "lineNo": 21,
        "endLineNo": 34,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fsearx%2Fsearx.py%23L21-L34&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Searx API Key Retrieval Function \n\n**[Quick Summary]** This Python function aims to retrieve a Searx API key from the `SEARX_URL` environment variable. If the variable is not set, it raises an error prompting the user to set it and provides a link to obtain a key.\n\n**[Inputs]**\n\n* `os.environ[\"SEARX_URL\"]`:  Environment variable expected to contain the Searx API key.\n\n\n\n**[Outputs]**\n\n* A string containing the Searx API key. \n* An Exception if the `SEARX_URL` environment variable is not found.  \n\n\n\n\n"
    },
    "gpt_researcher__retrievers__tavily__tavily_search__TavilySearch__get_api_key": {
        "label": "get_api_key",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/tavily/tavily_search.py",
        "relativePath": "gpt_researcher/retrievers/tavily/tavily_search.py",
        "lineNo": 29,
        "endLineNo": 42,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Ftavily%2Ftavily_search.py%23L29-L42&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Tavily API Key Retrieval Function\n\n**Quick Summary**\n\nThis function aims to retrieve the Tavily API key. It first looks for the key within request headers, then in the environment variable `TAVILY_API_KEY`, raising an exception if neither is found.\n\n**Inputs**\n\n*  `self.headers`:  This likely represents a dictionary-like object containing HTTP request headers.\n\n**Output**\n\n*  `api_key`: A string containing the Tavily API key. \n\n\n\n"
    },
    "gpt_researcher__scraper__beautiful_soup__beautiful_sou__BeautifulSoupScraper__get_content_from_url": {
        "label": "get_content_from_url",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/beautiful_soup/beautiful_soup.py",
        "relativePath": "gpt_researcher/scraper/beautiful_soup/beautiful_soup.py",
        "lineNo": 40,
        "endLineNo": 53,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Fbeautiful_soup%2Fbeautiful_soup.py%23L40-L53&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function extracts all text content from specified paragraph and heading HTML tags (<p>, <h1>, <h2>, <h3>, <h4>, <h5>) within a given BeautifulSoup object.  Its purpose is to cleanly parse and retrieve textual information from HTML.\n\n## Inputs\n\n* `soup` : A BeautifulSoup object representing an HTML document.\n\n## Output\n\n*  A string containing all the extracted text from the specified tags, with each piece of text separated by a new line character (\"\\n\").  \n\n\n"
    },
    "gpt_researcher__config__config__Config__parse_retrievers": {
        "label": "parse_retrievers",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/config/config.py",
        "relativePath": "gpt_researcher/config/config.py",
        "lineNo": 47,
        "endLineNo": 59,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fconfig%2Fconfig.py%23L47-L59&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "[Quick summary]\nThis function processes a string of comma-separated retriever names, ensuring they are valid. \n\nIt checks the input against a predefined list of acceptable retrievers and raises an error if any invalid names are found.\n\nIts purpose is to validate user-provided retriever configurations.\n\n[Inputs]\n* `retriever_str`: A string containing comma-separated retriever names. \n\n\n[Output]\n* A list of validated retriever names. \n"
    },
    "gpt_researcher__master__actions__table_of_contents__generate_table_of_contents": {
        "label": "generate_table_of_contents",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 582,
        "endLineNo": 594,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L582-L594&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Code Analysis \n\n**Quick Summary**\n\nThis function recursively generates a textual table of contents (TOC) from a markdown document. It takes a list of header elements (\"headers\") and indents them based on their hierarchy, creating a nested structure.\n\n**Inputs**\n\n* `headers`: A list of dictionaries, where each dictionary represents a markdown header and contains at least a \"text\" key for the header content.\n* `indent_level`: An integer indicating the current indentation level in the TOC.\n\n**Output**\n\n* `toc`: A multi-line string containing the generated table of contents. \n"
    },
    "gpt_researcher__master__agent__GPTResearcher__write_introduction": {
        "label": "write_introduction",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 426,
        "endLineNo": 438,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L426-L438&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function constructs a report introduction based on the provided query, context, role, configuration, websocket connection, and whether to include cost information. It aims to generate a concise and informative opening for a report.\n\n## Inputs\n\n* `self.query`: Likely the main topic or research question of the report.\n* `self.context`: Additional background information relevant to the query.\n* `self.role`:  The role or persona of the user requesting the report.\n* `self.cfg`:  A configuration object containing settings or parameters.\n* `self.websocket`: A websocket connection potentially used for real-time updates or interactions.\n* `self.add_costs`: A boolean flag indicating whether cost details should be included in the introduction.\n\n## Output\n\n* `introduction`: A string containing the generated report introduction. \n"
    },
    "gpt_researcher__retrievers__duckduckgo__duckduckgo__Duckduckgo__search": {
        "label": "search",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/duckduckgo/duckduckgo.py",
        "relativePath": "gpt_researcher/retrievers/duckduckgo/duckduckgo.py",
        "lineNo": 15,
        "endLineNo": 27,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fduckduckgo%2Fduckduckgo.py%23L15-L27&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function performs a DuckDuckGo search based on a given query. It fetches results from DuckDuckGo's API, limiting the response to a specified number of results. If there's an error during the search, it returns an empty list.\n\n## Inputs\n\n* **query:** The search term to be queried on DuckDuckGo.\n* **max_results:** The maximum number of search results to be returned.  \n\n## Output\n\n*  A list of search results fetched from DuckDuckGo. \n*  An empty list if an error occurs during the search process. \n"
    },
    "gpt_researcher__retrievers__exa__exa__ExaSearch____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/exa/exa.py",
        "relativePath": "gpt_researcher/retrievers/exa/exa.py",
        "lineNo": 10,
        "endLineNo": 22,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fexa%2Fexa.py%23L10-L22&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## ExaSearch Initialization Function Breakdown\n\n**Quick Summary**\n\nThis function initializes an ExaSearch object, which can be used to perform searches using the Exa platform. It takes a search query as input, retrieves an API key, and creates an Exa client instance for interacting with the API.  \n\n**Inputs**\n\n* `query`: The search query to be executed on the Exa platform. \n\n**Output**\n\n* Creates an object (`self`) with attributes:\n    * `query`: Stores the initial search query.\n    * `api_key`:  Holds the retrieved API key for authentication.\n    * `client`: An instance of the Exa client to interact with the API.  \n\n\n\nLet me know if you have any other code snippets you'd like me to analyze!\n"
    },
    "gpt_researcher__retrievers__serpapi__serpapi__SerpApiSearch__get_api_key": {
        "label": "get_api_key",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/serpapi/serpapi.py",
        "relativePath": "gpt_researcher/retrievers/serpapi/serpapi.py",
        "lineNo": 22,
        "endLineNo": 34,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fserpapi%2Fserpapi.py%23L22-L34&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## SerpApi API key Retriever Function Analysis\n\n**[Quick Summary]** This function attempts to retrieve your SerpApi API key from the environment variable \"SERPAPI_API_KEY\". It raises an exception if the key is not found, prompting you to set the environment variable. \n\n**[Inputs]**\n\n* **os.environ[\"SERPAPI_API_KEY\"]**: This is the name of the environment variable where your SerpApi API key is expected to be stored.\n\n* **\"SERPAPI_API_KEY\"**: This is a string literal used to access the specific environment variable.\n\n\n**[Output]**\n\n* **api_key**:  A string containing the value of your SerpApi API key, if it is found in the environment variable. \n"
    },
    "gpt_researcher__retrievers__serper__serper__SerperSearch__get_api_key": {
        "label": "get_api_key",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/serper/serper.py",
        "relativePath": "gpt_researcher/retrievers/serper/serper.py",
        "lineNo": 22,
        "endLineNo": 34,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fserper%2Fserper.py%23L22-L34&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "**Quick Summary**\n\nThis function retrieves the Serper API key from an environment variable. Its purpose is to securely store and access the API key required for interacting with the Serper API.\n\n**Inputs**\n\n* **os.environ[\"SERPER_API_KEY\"]**: An environment variable containing the Serper API key.\n\n**Output**\n\n* **api_key**: A string representing the Serper API key. \n\n\n"
    },
    "gpt_researcher__scraper__arxiv__arxiv__ArxivScraper__scrape": {
        "label": "scrape",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/arxiv/arxiv.py",
        "relativePath": "gpt_researcher/scraper/arxiv/arxiv.py",
        "lineNo": 10,
        "endLineNo": 22,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Farxiv%2Farxiv.py%23L10-L22&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function retrieves the content of the first document from Arxiv for a given query.  It uses the ArxivRetriever library to search for documents based on a query extracted from a provided link. The purpose of the code is to fetch and process research papers from Arxiv based on a specific topic or keyword.\n\n## Inputs\n\n*  `self.link`: A URL link referencing a research paper or topic on Arxiv. \n\n\n## Output\n\n*  `docs[0].page_content`: The text content of the first retrieved document. \n"
    },
    "gpt_researcher__master__actions__summarize__handle_task": {
        "label": "handle_task",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 288,
        "endLineNo": 299,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L288-L299&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Please provide the code selection you'd like me to analyze.  \n\n"
    },
    "gpt_researcher__master__prompts__generate_subtopic_report_prompt": {
        "label": "generate_subtopic_report_prompt",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/prompts.py",
        "relativePath": "gpt_researcher/master/prompts.py",
        "lineNo": 233,
        "endLineNo": 244,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fprompts.py%23L233-L244&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Let's break down this function.\n\n**Quick Summary**\n\nThis function appears designed to generate structured text content, likely an academic report section, based on provided input.  It takes information like the current subtopic, existing headers, and relevant content to create a formatted piece within a specified word count and tone.\n\n**Inputs**\n\n*  `current_subtopic`: The specific focus within the larger topic.\n*  `existing_headers`:  Possibly a list of section headings already established in the report.\n*  `relevant_written_contents`:  Content that is relevant to the subtopic and should be incorporated.\n*  `main_topic`: The overall subject of the report.\n*  `context`:  Additional background or information that might influence the generated text.\n*  `report_format`: The desired citation style (e.g., APA).\n*  `max_subsections`:  A limit on the number of sub-sections within this part of the report.\n*  `total_words`:  The target length of the generated text. \n*  `tone`: The desired writing style (e.g., objective, informal).\n\n**Output**\n\n*  A formatted string of text representing a section of the report.  \n\n\n\nLet me know if you have any other questions!\n"
    },
    "gpt_researcher__master__prompts__get_prompt_by_report_type": {
        "label": "get_prompt_by_report_type",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/prompts.py",
        "relativePath": "gpt_researcher/master/prompts.py",
        "lineNo": 358,
        "endLineNo": 369,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fprompts.py%23L358-L369&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Summary:\n\nThis function takes a `report_type` as input and uses a mapping to determine the appropriate prompt to use. If the provided `report_type` is invalid, it uses a default report type and warns the user. \n\n## Inputs:\n\n* `report_type`: A string representing the type of report.\n\n* `report_type_mapping`: A dictionary mapping report types to prompts. \n\n* `ReportType.ResearchReport.value`:  A constant likely representing the default report type.\n\n## Output:\n\n* `prompt_by_type`: The string corresponding to the prompt based on the input `report_type`. \n\n\n\n\nLet me know if you have any other code snippets you'd like me to analyze!\n"
    },
    "gpt_researcher__retrievers__bing__bing__BingSearch__get_api_key": {
        "label": "get_api_key",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/bing/bing.py",
        "relativePath": "gpt_researcher/retrievers/bing/bing.py",
        "lineNo": 22,
        "endLineNo": 33,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fbing%2Fbing.py%23L22-L33&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Analysis:\n\n**Quick Summary:** \n\nThis function retrieves the Bing API key from the environment variable \"BING_API_KEY\".  It's purpose is to safely store and access the key required for interacting with the Bing API. If the key is not found in the environment, it raises an exception.\n\n**Inputs:**\n\n* **os.environ[\"BING_API_KEY\"]**: This accesses the value stored in the \"BING_API_KEY\" environment variable.\n\n**Output:**\n\n* **api_key**:  The Bing API key as a string.\n\n\n\n\nLet me know if you'd like me to elaborate on any aspect!\n"
    },
    "gpt_researcher__scraper__pymupdf__pymupdf__PyMuPDFScraper__scrape": {
        "label": "scrape",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/pymupdf/pymupdf.py",
        "relativePath": "gpt_researcher/scraper/pymupdf/pymupdf.py",
        "lineNo": 10,
        "endLineNo": 21,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Fpymupdf%2Fpymupdf.py%23L10-L21&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Function Breakdown \n\n**[Quick Summary]** \n\nThis function, `scrape`, downloads a PDF document from a given link using `PyMuPDFLoader` and returns its entire content as a single string. Its purpose is likely to extract all the text from a PDF file for further processing.\n\n**[Inputs]**\n\n*  `self.link`: This refers to a link (presumably a URL) pointing to the PDF document to be downloaded.\n\n**[Output]**\n\n*  A string: This string contains the entire textual content of the downloaded PDF document. \n\n\n"
    },
    "gpt_researcher__master__prompts__get_report_by_type": {
        "label": "get_report_by_type",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/prompts.py",
        "relativePath": "gpt_researcher/master/prompts.py",
        "lineNo": 152,
        "endLineNo": 162,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fprompts.py%23L152-L162&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Breakdown\n\n**Quick Summary:** This function determines the appropriate prompt to generate based on the specified `report_type`.  It acts as a dispatcher, selecting the correct function (`generate_report_prompt`, etc.) to create the prompt for the desired type of report.\n\n**Inputs:**\n* `report_type`:  A value representing the type of report to be generated (e.g., ResearchReport, ResourceReport). \n\n**Output:**\n* A function that generates a prompt tailored to the specific `report_type`. \n"
    },
    "gpt_researcher__retrievers__google__google__GoogleSearch____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/google/google.py",
        "relativePath": "gpt_researcher/retrievers/google/google.py",
        "lineNo": 13,
        "endLineNo": 23,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fgoogle%2Fgoogle.py%23L13-L23&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  TavilySearch Initialization Code Summary\n\nThis code initializes a TavilySearch object, likely used to interact with a search engine API. It takes a user query and optional headers containing an API key and custom search engine ID, falling back to environment variables if not provided.\n\n \n## Inputs\n\n* `query`: The search query entered by the user.\n* `headers`: A dictionary of custom headers that may contain:\n    * `\"google_api_key\"`:  API key for authentication with the search engine.\n    * `\"google_cx_key\"`: Custom search engine ID for targeted results.\n\n## Output\n\n* An initialized `TavilySearch` object ready for use.  \n"
    },
    "gpt_researcher__retrievers__searx__searx__SearxSearch__search": {
        "label": "search",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/searx/searx.py",
        "relativePath": "gpt_researcher/retrievers/searx/searx.py",
        "lineNo": 35,
        "endLineNo": 45,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fsearx%2Fsearx.py%23L35-L45&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function performs a search query using a Searx instance, extracts the link and snippet for each result, and returns them in a standardized format.  Its purpose is to provide a common interface for integrating search functionality from Searx with other applications.\n\n## Inputs\n\n*  `self.query`: The search query to be executed.\n*  `max_results`: The maximum number of search results to return. \n*  `os.environ[\"SEARX_URL\"]`: The URL of the Searx instance to use for the search. \n\n## Output\n\n* `search_response`: A list of dictionaries, where each dictionary contains:\n    * `href`: The URL of the search result.\n    * `body`: A brief summary or snippet of the search result. \n\n"
    },
    "gpt_researcher__retrievers__semantic_scholar__semantic_scholar__SemanticScholarSearch____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/semantic_scholar/semantic_scholar.py",
        "relativePath": "gpt_researcher/retrievers/semantic_scholar/semantic_scholar.py",
        "lineNo": 14,
        "endLineNo": 24,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fsemantic_scholar%2Fsemantic_scholar.py%23L14-L24&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## SemanticScholarSearch Initialization Function Summary & Details\n\n**[Quick Summary]**\nThis function initializes an object for searching Semantic Scholar using a provided query and sort criterion. It ensures the sort criterion is valid before proceeding. The purpose is to set up a searchable object tailored to specific search needs.\n\n**[Inputs]**\n* `query`: A string representing the search term or phrase to be used on Semantic Scholar.\n* `sort`: A string indicating the desired sorting order for search results (relevance, citation count, publication date).\n\n**[Output]**\n* An initialized `SemanticScholarSearch` object, ready for use.  \n"
    },
    "gpt_researcher__scraper__scraper__Scraper____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/scraper.py",
        "relativePath": "gpt_researcher/scraper/scraper.py",
        "lineNo": 19,
        "endLineNo": 29,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Fscraper.py%23L19-L29&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Python Scraper Code Analysis\n\n**Quick Summary**\n\nThis function initializes a web scraper object. It sets up a `requests` session with a specified user agent and initializes a scraper object (`self.scraper`). The intended purpose is to facilitate web scraping by handling session management and user agent spoofing.  \n\n**Inputs**\n\n* `urls`: This is likely a list or iterable of URLs that the scraper will target. \n* `user_agent`:  A string representing a user agent header, used to mimic a web browser and avoid detection by websites.\n\n\n**Output**\n\n* An instance of the `Scraper` class is created and ready to scrape the provided URLs. \n"
    },
    "gpt_researcher__document__langchain_document__LangChainDocumentLoader__load": {
        "label": "load",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/document/langchain_document.py",
        "relativePath": "gpt_researcher/document/langchain_document.py",
        "lineNo": 15,
        "endLineNo": 24,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fdocument%2Flangchain_document.py%23L15-L24&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Quick Summary\nThis function processes a list of documents and generates a list of dictionaries. Each dictionary contains the raw content of a document and its URL (if available) extracted from metadata. The purpose is likely to prepare document data for further processing or storage.\n\n## Inputs\n*  `self.documents`: A list of document objects, each presumably containing page content and metadata.\n\n## Output\n*  `docs`: A list of dictionaries, where each dictionary represents a document with the following keys:\n    * `\"raw_content\"`: The text content of the document.\n    * `\"url\"`: The URL of the document (empty string if not available). \n\n\n"
    },
    "gpt_researcher__llm_provider__generic__base__GenericLLMProvider__get_chat_response": {
        "label": "get_chat_response",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/llm_provider/generic/base.py",
        "relativePath": "gpt_researcher/llm_provider/generic/base.py",
        "lineNo": 94,
        "endLineNo": 103,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fllm_provider%2Fgeneric%2Fbase.py%23L94-L103&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary:\n\nThis function handles the response from a large language model (LLM), choosing between synchronous and asynchronous retrieval based on the presence of a `stream` variable. If no stream is provided, it uses `ainvoke` for a direct, blocking response. Otherwise, it utilizes `stream_response` for a streaming response.  \n\n## Inputs:\n\n* `messages`: A likely list or array of prompts or messages to be sent to the LLM. \n* `stream`: A boolean variable indicating whether a streaming response is desired.\n* `websocket`:  A websocket connection potentially used for streaming responses.\n\n\n## Output:\n\n* The output of the LLM, which could be:\n    * A string containing the model's response (synchronous case).\n    * A streaming data source (asynchronous case). \n"
    },
    "gpt_researcher__retrievers__custom__custom__CustomRetriever___populate_params": {
        "label": "_populate_params",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/custom/custom.py",
        "relativePath": "gpt_researcher/retrievers/custom/custom.py",
        "lineNo": 19,
        "endLineNo": 28,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fcustom%2Fcustom.py%23L19-L28&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "**[Quick Summary]**  This Python function extracts environment variables whose names begin with 'RETRIEVER_ARG_' and returns them as a dictionary.  It's likely used for configuring a program or system component using external settings. \n\n**[Inputs]**\n\n* `os.environ`:  A dictionary-like object representing all the environment variables set on the system.\n\n**[Output]**\n\n* A dictionary where:\n    * Keys are the names of the retrieved environment variables, without the 'RETRIEVER_ARG_' prefix, and converted to lowercase.\n    * Values are the corresponding values of those environment variables. \n\n\n"
    },
    "gpt_researcher__scraper__scraper__Scraper__run": {
        "label": "run",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/scraper.py",
        "relativePath": "gpt_researcher/scraper/scraper.py",
        "lineNo": 30,
        "endLineNo": 39,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Fscraper.py%23L30-L39&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function retrieves content from a list of URLs concurrently using a thread pool. It calls a `extract_data_from_link` function for each URL, likely to parse and extract specific information.  The results are then filtered to only include entries with successfully extracted content.\n\n## Inputs\n\n* `self.urls`: A list of URLs to extract content from.\n* `self.session`: Likely a library-provided session object for handling HTTP requests (e.g., from `requests`).\n\n## Output\n\n* A list of dictionaries, where each dictionary represents a URL and contains at least a key \"raw_content\" holding the extracted content for that URL. \n\n\n"
    },
    "gpt_researcher__master__agent__GPTResearcher__add_costs": {
        "label": "add_costs",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 417,
        "endLineNo": 425,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L417-L425&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "[Quick Summary] \nThis function appears to be a method within a class (likely dealing with research or development),  that adds a cost to a running total of research costs.  It ensures the input cost is either an integer or a float, raising a ValueError if it's not. The purpose is to maintain a cumulative record of research expenses.\n\n[Inputs]\n-  `cost`: This is the numerical value representing the cost to be added.\n\n\n[Output]\n- The `self.research_costs` attribute is updated to include the provided `cost`. \n"
    },
    "gpt_researcher__retrievers__bing__bing__BingSearch____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/bing/bing.py",
        "relativePath": "gpt_researcher/retrievers/bing/bing.py",
        "lineNo": 13,
        "endLineNo": 21,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fbing%2Fbing.py%23L13-L21&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## BingSearch Initialization \n\n**[Quick summary]**  This function initializes a BingSearch object, which is likely used to perform searches on the Bing search engine. It takes a user's search query as input and retrieves an API key for authentication. \n\n**[Inputs]**\n* `query`: The search query entered by the user.\n\n**[Output]**\n* Initializes a `BingSearch` object with:\n    * The user's search `query`.\n    * A retrieved `api_key` for Bing API access. \n\n\n"
    },
    "gpt_researcher__retrievers__pubmed_central__pubmed_central__PubMedCentralSearch____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/pubmed_central/pubmed_central.py",
        "relativePath": "gpt_researcher/retrievers/pubmed_central/pubmed_central.py",
        "lineNo": 12,
        "endLineNo": 20,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fpubmed_central%2Fpubmed_central.py%23L12-L20&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Python Code Analysis\n\n**Quick Summary**\n\nThis code snippet initializes a PubMedCentralSearch object, which likely facilitates searching the PubMed Central database. It takes a search query as input and retrieves an API key for communication with the database. \n\n**Inputs**\n\n* `query`: The search term or phrase to be used in the PubMed Central search.\n\n* `self._retrieve_api_key()`: A method call that retrieves an API key necessary for interacting with the PubMed Central API.\n\n**Output**\n\n* Initializes a `self.query` attribute to store the search query.\n* Initializes a `self.api_key` attribute to store the retrieved API key. \n\n\n"
    },
    "gpt_researcher__retrievers__searx__searx__SearxSearch____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/searx/searx.py",
        "relativePath": "gpt_researcher/retrievers/searx/searx.py",
        "lineNo": 12,
        "endLineNo": 20,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fsearx%2Fsearx.py%23L12-L20&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## TavilySearch Initialization Function Breakdown\n\n**[Quick Summary]** This function initializes a TavilySearch object, which likely interacts with a search API. It stores the user's search query and retrieves an API key, preparing for API communication.  The purpose is to set up an object ready to perform searches using the TavilySearch API. \n\n**[Inputs]**\n* `query`: The user's search term.\n\n**[Outputs]**\n*  A `TavilySearch` object with the `query` and `api_key` attributes set.\n\n\n"
    },
    "gpt_researcher__retrievers__serpapi__serpapi__SerpApiSearch____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/serpapi/serpapi.py",
        "relativePath": "gpt_researcher/retrievers/serpapi/serpapi.py",
        "lineNo": 13,
        "endLineNo": 21,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fserpapi%2Fserpapi.py%23L13-L21&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Let's break down this Python code snippet:\n\n**Quick Summary**\n\nThis function initializes an object called `SerpApiSearch`, designed to interact with SerpApi (likely a search engine API). It takes a search `query` as input and sets it, along with fetching an API key, presumably for authentication with SerpApi.\n\n**Inputs**\n\n*  `query`: The search term or phrase you want to query SerpApi with.\n *  \n\n**Output**\n\n*   Creates an instance of a `SerpApiSearch` object.\n\n\n\n"
    },
    "gpt_researcher__retrievers__serper__serper__SerperSearch____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/serper/serper.py",
        "relativePath": "gpt_researcher/retrievers/serper/serper.py",
        "lineNo": 13,
        "endLineNo": 21,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fserper%2Fserper.py%23L13-L21&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  SerperSearch Initialization\n\n**Summary:**\n\nThis function initializes a SerperSearch object, storing the user's search query and fetching an API key. Its purpose is to prepare the object for interacting with the Serper API to perform a search.\n\n**Inputs:**\n\n* `query`: The search term the user wants to look for.\n\n**Outputs:**\n\n*  A SerperSearch object with the `query` and `api_key` attributes set. \n\n\n\nLet me know if you'd like me to elaborate on any of these points! \n"
    },
    "gpt_researcher__utils__costs__estimate_llm_cost": {
        "label": "estimate_llm_cost",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/utils/costs.py",
        "relativePath": "gpt_researcher/utils/costs.py",
        "lineNo": 12,
        "endLineNo": 20,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Futils%2Fcosts.py%23L12-L20&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Function Breakdown \n\n**Quick Summary:** This function calculates the cost of encoding both input and output content using a specified tokenization model. It multiplies the token count of each content piece by its respective cost per token and returns their sum, representing the total encoding cost.\n\n**Inputs:**\n\n*  `ENCODING_MODEL`:  The name or identifier of the tokenization model to use.\n* `input_content`: The text content to be encoded into tokens.\n* `output_content`: The text content to be encoded into tokens.\n* `INPUT_COST_PER_TOKEN`: Cost assigned to each token in the input content.\n* `OUTPUT_COST_PER_TOKEN`: Cost assigned to each token in the output content.\n\n**Output:**\n\n* A single value representing the total cost of encoding both the input and output content. \n\n\n\n"
    },
    "gpt_researcher__utils__enum__ReportType": {
        "label": "ReportType",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/utils/enum.py",
        "relativePath": "gpt_researcher/utils/enum.py",
        "lineNo": 4,
        "endLineNo": 12,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Futils%2Fenum.py%23L4-L12&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Code Analysis\n\n**Quick Summary:** \n\nThis code defines a set of named constants, likely representing different report types within a system. These constants are used to identify and categorize various reports, possibly for easy management, filtering, or display purposes.\n\n**Inputs:**\n\n*  None (This is a definition of constants, not a function call)\n\n**Output:**\n\n*  Six named constants:\n    * `ResearchReport`\n    * `ResourceReport`\n    * `OutlineReport`\n    * `CustomReport`\n    * `DetailedReport`\n    * `SubtopicReport` \n\n\n"
    },
    "gpt_researcher__config__config__Config__load_config_file": {
        "label": "load_config_file",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/config/config.py",
        "relativePath": "gpt_researcher/config/config.py",
        "lineNo": 64,
        "endLineNo": 71,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fconfig%2Fconfig.py%23L64-L71&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Analysis \n\n**Quick Summary:**\nThis Python function loads configuration data from a JSON file specified by `self.config_file`. It parses the JSON content, extracts key-value pairs, and assigns these values as attributes to the current object (`self`). \n\n**Inputs:**\n\n*  `self.config_file`: A string representing the path to the JSON configuration file.\n\n**Output:**\n*  None: The function likely doesn't return a specific value but modifies the object's attributes directly. \n\n\n"
    },
    "gpt_researcher__context__compression__ContextCompressor__async_get_context": {
        "label": "async_get_context",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/compression.py",
        "relativePath": "gpt_researcher/context/compression.py",
        "lineNo": 52,
        "endLineNo": 59,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fcompression.py%23L52-L59&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Breakdown:\n\n**Quick Summary:**\nThis function retrieves relevant documents from a collection based on a given query. It uses a contextual retriever (likely a language model) to generate embeddings for both the query and documents, allowing for semantic similarity matching.  It also provides an estimated cost for the embedding generation. \n\n**Inputs:**\n\n*  `self.__get_contextual_retriever()`:  A function call that likely returns an instance of a contextual document retriever (e.g., based on OpenAI's embedding model).\n* `query`: The text input for which relevant documents are sought.\n* `cost_callback`: A function (optional) that takes the estimated embedding cost as input.\n* `max_results`: The maximum number of relevant documents to return.\n\n**Output:**\n\n*  `self.__pretty_print_docs(relevant_docs, max_results)`: A formatted list of the most relevant documents found,  limited to `max_results`.  \n\n\n\n"
    },
    "gpt_researcher__retrievers__custom__custom__CustomRetriever____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/custom/custom.py",
        "relativePath": "gpt_researcher/retrievers/custom/custom.py",
        "lineNo": 11,
        "endLineNo": 18,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fcustom%2Fcustom.py%23L11-L18&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Breakdown:\n\n**Quick Summary:**\n\nThis function snippet initializes a retriever object, likely for fetching data from a remote endpoint. It fetches the `RETRIEVER_ENDPOINT` from the environment, validates its presence, constructs query parameters, and stores a user-provided `query`.\n\n**Inputs:**\n\n* `query`: A string representing the search or data request to be sent to the endpoint.\n\n**Output:**\n\n* None (implied initialization and preparation for subsequent data retrieval) \n\n\n"
    },
    "gpt_researcher__utils__enum__ReportSource": {
        "label": "ReportSource",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/utils/enum.py",
        "relativePath": "gpt_researcher/utils/enum.py",
        "lineNo": 13,
        "endLineNo": 20,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Futils%2Fenum.py%23L13-L20&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Analysis\n\n**Quick Summary:** This code snippet appears to define a set of constants representing different data sources or modalities. These constants likely serve as identifiers or flags when working with diverse data types, potentially within a text processing or knowledge management system.\n\n**Inputs:**  None. This code only defines constants.\n\n**Output:**  \n* `Web`: Represents data originating from the internet.\n* `Local`: Represents data stored locally on a system.\n* `LangChainDocuments`: Likely refers to documents handled by the LangChain library, a framework for developing applications with large language models.\n* `Sources`:  A general constant possibly indicating the origin of data.\n* `Hybrid`: Suggests a combination of different data sources. \n\n\n"
    },
    "gpt_researcher__context__compression__ContextCompressor____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/compression.py",
        "relativePath": "gpt_researcher/context/compression.py",
        "lineNo": 17,
        "endLineNo": 23,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fcompression.py%23L17-L23&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Let's break down the code snippet you provided.\n\n## Quick Summary\n\nThis code snippet initializes a class or object designed for semantic search. It takes a set of documents, embeds them, and allows for searching for similar documents based on input queries. The `similarity_threshold` determines the level of similarity required for a document to be considered a match.\n\n## Inputs\n\n* **`max_results`:** The maximum number of relevant documents to return in a search result.\n* **`documents`:** A collection of text documents to be indexed and searched.\n* **`kwargs`:**  A dictionary of keyword arguments that can be used to configure the object's behavior. \n* **`embeddings`:** A pre-computed set of vector representations (embeddings) for each document in the `documents` collection.\n* **`os.environ.get(\"SIMILARITY_THRESHOLD\", 0.38)`:**  Retrieves the similarity threshold from an environment variable. If not found, defaults to 0.38.\n\n## Output\n\n* A search interface that can take input queries and return a ranked list of  \ndocuments most similar to the query, up to the specified `max_results`.  \n\n\nLet me know if you have any other questions!\n"
    },
    "gpt_researcher__context__compression__ContextCompressor__get_context": {
        "label": "get_context",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/compression.py",
        "relativePath": "gpt_researcher/context/compression.py",
        "lineNo": 45,
        "endLineNo": 51,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fcompression.py%23L45-L51&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis\n\n**Quick Summary:** This function retrieves relevant documents from a collection based on a given query. It first estimates the cost of embedding the documents using a specified OpenAI model and optionally calls a callback function to inform about this cost. Then, it uses a contextual retriever to find the most relevant documents and returns them in a prettified format, limiting the results to a maximum number.\n\n**Inputs:**\n\n* `query`: The text query used to search for relevant documents.\n* `cost_callback`:  A function that can be called to estimate and handle the cost of embedding the documents.\n* `max_results`: The maximum number of relevant documents to return.\n\n**Output:**\n* A prettified list of the most relevant documents found based on the query. \n\n\n\n"
    },
    "gpt_researcher__context__compression__WrittenContentCompressor__get_context": {
        "label": "get_context",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/compression.py",
        "relativePath": "gpt_researcher/context/compression.py",
        "lineNo": 85,
        "endLineNo": 91,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fcompression.py%23L85-L91&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "[Quick summary]\nThis function retrieves relevant documents from a collection based on a given query. It utilizes a contextual retriever model for efficient document search and optionally estimates the embedding cost for the query.\n\n[Inputs]\n- self: Likely refers to an instance of a class containing the document collection and retriever.\n- query: The user's search query.\n- max_results: The maximum number of relevant documents to return.\n- cost_callback: A function to be called if provided, which estimates the cost of retrieving embeddings.\n- OPENAI_EMBEDDING_MODEL: A pre-defined name or identifier for an embedding model.\n\n[Output]\n- A list of relevant documents, formatted for display or further processing. \n\n\n"
    },
    "gpt_researcher__llm_provider__generic__base___check_pkg": {
        "label": "_check_pkg",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/llm_provider/generic/base.py",
        "relativePath": "gpt_researcher/llm_provider/generic/base.py",
        "lineNo": 141,
        "endLineNo": 147,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fllm_provider%2Fgeneric%2Fbase.py%23L141-L147&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Breakdown\n\n**Quick Summary:**  This code snippet checks if a Python package (`pkg`) is installed. If not, it raises an `ImportError` prompting the user to install it using `pip`.  Essentially, it ensures the necessary package is available before proceeding with the code.\n\n**Inputs:**\n\n*  `pkg`: A string representing the name of the Python package to import.\n\n**Output:**\n\n*  If `pkg` is installed: No output, the code continues execution.\n* If `pkg` is not installed: An `ImportError` is raised with a message instructing the user to install the package using `pip install -U {pkg_kebab}`. \n\n\n\n"
    },
    "gpt_researcher__master__actions__extract_json_with_regex": {
        "label": "extract_json_with_regex",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 183,
        "endLineNo": 189,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L183-L189&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Analysis \n\n**[Quick summary]** This function aims to extract JSON data from a given response string. It uses a regular expression to search for a pattern matching  JSON (enclosed in curly braces) and returns the matched JSON string if found, otherwise it returns `None`.\n\n**[Inputs]**\n\n*  `response`: A string containing potentially  embedded JSON data.\n\n**[Output]**\n\n* A string containing the matched JSON data, or `None` if no JSON is found. \n\n\n"
    },
    "gpt_researcher__master__prompts__generate_draft_titles_prompt": {
        "label": "generate_draft_titles_prompt",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/prompts.py",
        "relativePath": "gpt_researcher/master/prompts.py",
        "lineNo": 305,
        "endLineNo": 311,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fprompts.py%23L305-L311&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "[Quick summary]\nThis function generates an HTML structure for an outline, potentially for a web page or documentation, based on given topic information. It organizes the content into sections based on the `current_subtopic` and limits the number of subsections to `max_subsections`.\n\n[Inputs]\n* `current_subtopic`: The current subtopic being outlined.\n* `main_topic`: The overarching topic of the outline.\n* `context`:  Additional contextual information that might influence the structure.\n* `max_subsections`: The maximum number of subsections allowed.\n\n[Output]\n* A formatted string containing HTML code for an outline.\n\n\n\n\n"
    },
    "gpt_researcher__retrievers__arxiv__arxiv__ArxivSearch____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/arxiv/arxiv.py",
        "relativePath": "gpt_researcher/retrievers/arxiv/arxiv.py",
        "lineNo": 8,
        "endLineNo": 14,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Farxiv%2Farxiv.py%23L8-L14&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Breakdown\n\n**Quick Summary:**  This code snippet initializes an object designed to search the arXiv preprint server. It takes an arXiv object, a search query, and a sort criterion (Relevance or SubmittedDate) as input, then sets up the search parameters accordingly. The purpose is to streamline the process of searching arXiv for relevant scientific papers.\n\n**Inputs:**\n\n* `arxiv`: An object likely representing a connection or interface to the arXiv API.\n* `query`: A string containing the search terms to be used in the arXiv search.\n* `sort`: A string specifying the sorting order for search results ('Relevance' or 'SubmittedDate').\n\n**Output:**\n\n* An initialized object configured to perform an arXiv search based on the provided parameters. \n\n\n"
    },
    "gpt_researcher__retrievers__utils__check_pkg": {
        "label": "check_pkg",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/utils.py",
        "relativePath": "gpt_researcher/retrievers/utils.py",
        "lineNo": 4,
        "endLineNo": 10,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Futils.py%23L4-L10&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary:\n\nThis Python function checks if a specified package (`pkg`) is installed. If not found, it generates a user-friendly error message suggesting installation using `pip` and raises an `ImportError`.\n\nThis code snippet ensures that a required package is available before attempting to use it, preventing runtime errors. \n\n## Inputs:\n* `pkg`: A string representing the name of the Python package to check for.\n\n## Outputs:\n* If the package is found: The function continues execution without raising any error.\n* If the package is not found: An `ImportError` is raised with a message indicating the missing package and instructions for installing it. \n\n\n\n\n"
    },
    "gpt_researcher__context__compression__ContextCompressor____pretty_print_docs": {
        "label": "__pretty_print_docs",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/compression.py",
        "relativePath": "gpt_researcher/context/compression.py",
        "lineNo": 39,
        "endLineNo": 44,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fcompression.py%23L39-L44&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function processes a list of documents (`docs`), extracts metadata like source and title, and combines them with the content to create a formatted string.  It then limits the output to the top `top_n` documents.  \n\nThis likely aims to generate concise summaries of a set of documents, highlighting key information.\n\n## Inputs\n\n* `docs`: A list of documents, where each document presumably has a `metadata` attribute containing source and title information, and a `page_content` attribute.\n* `top_n`: An integer specifying the maximum number of documents to include in the output.\n\n## Output\n\n* A formatted string containing the source, title, and content of the top `top_n` documents from the input list.  \n* Each document's information is separated by a newline character (`\\n`). \n\n\n"
    },
    "gpt_researcher__context__compression__WrittenContentCompressor____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/compression.py",
        "relativePath": "gpt_researcher/context/compression.py",
        "lineNo": 61,
        "endLineNo": 66,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fcompression.py%23L61-L66&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis\n\n[**Quick Summary**] \nThis Python code initializes an object likely used for document similarity search. It stores a collection of documents, keyword arguments (potential search parameters), pre-computed embeddings for the documents, and a threshold for determining similarity between documents. The purpose is to efficiently find documents that are semantically similar based on their embeddings.  \n\n[**Inputs**]\n* **documents:** A list or collection of documents to be analyzed.\n* **kwargs:** A dictionary of keyword arguments, potentially containing search criteria or parameters.\n* **embeddings:** A data structure (e.g., list or dictionary) containing pre-computed vector embeddings for each document.\n* **similarity_threshold:** A numerical value representing the minimum similarity score required for two documents to be considered similar.\n\n[**Output**]\n*  The initialized object itself, ready to be used for tasks like document retrieval or clustering. \n\n\n\n"
    },
    "gpt_researcher__context__compression__WrittenContentCompressor__async_get_context": {
        "label": "async_get_context",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/compression.py",
        "relativePath": "gpt_researcher/context/compression.py",
        "lineNo": 92,
        "endLineNo": 97,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fcompression.py%23L92-L97&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis:\n\n**Quick summary:** This function efficiently retrieves relevant documents from a collection based on a given query. It leverages a contextual retriever (likely an embedding-based system) to find similar documents and returns a nicely formatted list of the most relevant ones. The `cost_callback` can be used to estimate the computational cost of the retrieval process.\n\n**Inputs:**\n\n* `self.documents`: A collection of documents to be searched.\n* `query`:  The text query used to search for relevant documents.\n* `max_results`:  The maximum number of relevant documents to return.\n* `cost_callback`: An optional callback function to estimate the cost of embedding generation.\n\n**Output:**\n\n*  A list of relevant documents, formatted for easy viewing. \n\n\n"
    },
    "gpt_researcher__master__actions__get_default_retriever": {
        "label": "get_default_retriever",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 113,
        "endLineNo": 118,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L113-L118&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Breakdown:  \n\n**Quick Summary:**  This function defines a function that returns an instance of the `TavilySearch` class, which is likely designed for information retrieval from a specialized dataset or knowledge base.  Its purpose is to provide a convenient way to access and utilize TavilySearch's capabilities for searching and retrieving relevant information. \n\n**Inputs:**\n\n*  None (This function takes no explicit inputs)\n\n**Output:**\n\n* An instance of the `TavilySearch` class. \n\n\n\nLet me know if you'd like a deeper dive into  `TavilySearch` or have any other code snippets you'd like analyzed!\n"
    },
    "gpt_researcher__master__actions__summarize__chunk_content": {
        "label": "chunk_content",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/actions.py",
        "relativePath": "gpt_researcher/master/actions.py",
        "lineNo": 300,
        "endLineNo": 305,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Factions.py%23L300-L305&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Analysis:\n\n\n**[Quick Summary]**\n\nThis Python function efficiently processes a large text input (`raw_content`) by splitting it into smaller chunks (`chunk_size`) and yielding each chunk as a separate string. This technique allows for parallel processing of text data, enhancing performance for large files. \n\n**[Inputs]**\n\n* `raw_content`: The text input to be processed.\n* `chunk_size`:  The desired size of each text chunk. \n\n**[Output]**\n\n* A sequence of strings, each representing a chunk of the original text. \n   \n"
    },
    "gpt_researcher__master__prompts__generate_custom_report_prompt": {
        "label": "generate_custom_report_prompt",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/prompts.py",
        "relativePath": "gpt_researcher/master/prompts.py",
        "lineNo": 128,
        "endLineNo": 133,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fprompts.py%23L128-L133&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Breakdown  \n\n**Quick Summary:**  \n\nThis function takes a context, a query prompt, and other parameters (source, format, word limit) and formats them into a specific string structure. It seems designed to prepare input for a system that processes text information and generates reports.  \n\n\n**Inputs:**\n\n* `query_prompt`: Likely the question or instruction to be answered or addressed by the system.\n* `context`: Background information or data related to the query.\n* `report_source`:  Possibly the origin or identifier of the source material used for the report.\n* `report_format`: Desired format for the generated report (e.g., APA for academic citations).\n* `total_words`:  Specifies a target word count for the report.\n\n\n**Output:**\n\n\n* A formatted string that includes the provided context followed by the query prompt, separated by a newline. \n"
    },
    "gpt_researcher__retrievers__duckduckgo__duckduckgo__Duckduckgo____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/retrievers/duckduckgo/duckduckgo.py",
        "relativePath": "gpt_researcher/retrievers/duckduckgo/duckduckgo.py",
        "lineNo": 9,
        "endLineNo": 14,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fretrievers%2Fduckduckgo%2Fduckduckgo.py%23L9-L14&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis \n\n**Quick Summary:** This code snippet initializes a DuckDuckGo search object and sets up a query to be used for searching. It likely intends to perform a search using the DuckDuckGo search engine. \n\n**Inputs:**\n\n*  `query`: A string representing the search query.  \n\n**Output:**\n\n* A DuckDuckGo search object (`self.ddg`) prepared to execute the given `query`. \n\n\n"
    },
    "gpt_researcher__utils__costs__estimate_embedding_cost": {
        "label": "estimate_embedding_cost",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/utils/costs.py",
        "relativePath": "gpt_researcher/utils/costs.py",
        "lineNo": 21,
        "endLineNo": 25,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Futils%2Fcosts.py%23L21-L25&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "##  Function Breakdown\n\n**Quick Summary:** This function calculates the estimated cost of embedding a set of documents using a given AI model. It does this by tokenizing each document, summing the token count, and then multiplying by a predefined cost per token.\n\n**Inputs:**\n\n*  `model`:  Represents the AI model used for embedding. \n*  `docs`: A collection of documents (presumably strings) that need to be embedded.\n\n**Output:**\n\n* A numerical value representing the estimated cost of embedding all the documents. \n\n\n"
    },
    "gpt_researcher__utils__llm__get_llm": {
        "label": "get_llm",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/utils/llm.py",
        "relativePath": "gpt_researcher/utils/llm.py",
        "lineNo": 17,
        "endLineNo": 21,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Futils%2Fllm.py%23L17-L21&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Breakdown:\n\n**[Quick Summary]**\n\nThis function creates an instance of the `GenericLLMProvider` class, which is likely a class used to interact with various language models (LLMs). It takes an `llm_provider` argument, which specifies the type of LLM to use, and any additional keyword arguments (`**kwargs`). \n\n**[Inputs]**\n\n* `llm_provider`: \n    * This is the type of LLM provider to be used. It could be a string representing a model name or a class object.\n* `**kwargs`:\n    * These are keyword arguments that provide additional configuration options for the `GenericLLMProvider` instance. This could include things like API keys, model parameters, or specific functionalities.\n\n**[Output]**\n\n* An instance of the `GenericLLMProvider` class, tailored to the specified `llm_provider` and configuration parameters.  \n\n\n"
    },
    "gpt_researcher__config__config__Config__validate_doc_path": {
        "label": "validate_doc_path",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/config/config.py",
        "relativePath": "gpt_researcher/config/config.py",
        "lineNo": 60,
        "endLineNo": 63,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fconfig%2Fconfig.py%23L60-L63&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary \n\nThis function creates a folder at the specified path if it doesn't already exist.  The purpose  is to ensure that a necessary directory is available for writing or reading documents.\n\n## Inputs\n\n* **self.doc_path:**  A string representing the desired directory path for the document.\n\n## Output\n\n* None (The function modifies the file system, not returning a direct value) \n"
    },
    "gpt_researcher__master__agent__GPTResearcher__get_similar_written_contents_by_draft_section_titles__process_query": {
        "label": "process_query",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 556,
        "endLineNo": 559,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L556-L559&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Please provide the code selection you'd like me to analyze. I need the text of the function to give you a summary, inputs, and expected output.  \n\n"
    },
    "gpt_researcher__scraper__arxiv__arxiv__ArxivScraper____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/arxiv/arxiv.py",
        "relativePath": "gpt_researcher/scraper/arxiv/arxiv.py",
        "lineNo": 6,
        "endLineNo": 9,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Farxiv%2Farxiv.py%23L6-L9&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "You provided a snippet of code:\n\n```python\n        self.link = link\n        self.session = session\n```\n\nHere's a breakdown:\n\n**[Quick Summary]**\n\nThis code snippet appears to be initializing an object (likely within a class). It assigns the value of the `link` variable to the object's `link` attribute and the value of the `session` variable to the object's `session` attribute. These attributes could represent a URL or a connection to a service.\n\n**[Inputs]**\n\n*  `link`:  A URL or some representation of a web address.\n*  `session`: A session object, potentially representing a user's active connection to a website or service.\n\n**[Output]**\n\n*  An object with initialized attributes: `link` and `session`. \n"
    },
    "gpt_researcher__scraper__beautiful_soup__beautiful_sou__BeautifulSoupScraper____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/beautiful_soup/beautiful_soup.py",
        "relativePath": "gpt_researcher/scraper/beautiful_soup/beautiful_soup.py",
        "lineNo": 6,
        "endLineNo": 9,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Fbeautiful_soup%2Fbeautiful_soup.py%23L6-L9&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Breakdown\n\n**Quick Summary:** \n\nThis code snippet likely initializes an object responsible for interacting with a web resource.  It sets up the object with a `link` (probably a URL) and a `session` object (potentially for managing authentication or browsing context). The purpose is to create an object ready to fetch and process data from the provided link.\n\n**Inputs:**\n\n* `link`: A string representing the URL to interact with. \n* `session`: An object likely containing information about the user's session or browsing context.\n\n**Output:**\n\n*  None. This code snippet appears to be a constructor, initializing an object's state rather than producing an immediate output.  \n\n\nLet me know if you'd like me to elaborate on any specific aspect! \n"
    },
    "gpt_researcher__scraper__pymupdf__pymupdf__PyMuPDFScraper____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/pymupdf/pymupdf.py",
        "relativePath": "gpt_researcher/scraper/pymupdf/pymupdf.py",
        "lineNo": 6,
        "endLineNo": 9,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Fpymupdf%2Fpymupdf.py%23L6-L9&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Let's break down this code snippet:\n\n**Quick Summary**\n\nThis function likely initializes an object for interacting with a web link using a provided web session. It sets instance variables 'link' and 'session' to store the target URL and the used web browsing session, respectively.\n\nThis is probably part of a larger system that handles web requests and interactions.\n\n**Inputs**\n\n* `link`:  Likely a string representing the URL of the webpage to interact with.\n* `session`:  Probably an object representing an established web browsing session. This could be a session maintained by a web browser library or framework. \n\n**Output**\n\n*  The function itself doesn't *return* a direct output. Instead, it initializes the object with the given link and session, making those values accessible for later use within the object's methods. \n\n\n\n\nLet me know if you want to explore a specific use case or have more code to analyze!\n"
    },
    "gpt_researcher__scraper__web_base_loader__web_base_loader__WebBaseLoaderScraper____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/scraper/web_base_loader/web_base_loader.py",
        "relativePath": "gpt_researcher/scraper/web_base_loader/web_base_loader.py",
        "lineNo": 4,
        "endLineNo": 7,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fscraper%2Fweb_base_loader%2Fweb_base_loader.py%23L4-L7&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Analysis:\n\n**Quick summary:**  This code snippet likely initializes an object (possibly a class instance) responsible for interacting with a web link using a specified session.  Its purpose seems to be setting up the connection and context for web requests or scraping.\n\n**Inputs:**\n\n*  `link`: Represents the URL or web address to interact with.\n*  `session`:  Likely a pre-configured web session object, possibly from a library like `requests` or `selenium`,  used to manage the interaction with the website.\n\n**Output:**\n\n*  The code doesn't directly produce an output. It primarily sets up internal variables within an object.  \n*  The expectation is that subsequent methods of this object will utilize the provided `link` and `session` to perform actions on the web. \n\n\n"
    },
    "gpt_researcher__context__compression__WrittenContentCompressor____pretty_docs_list": {
        "label": "__pretty_docs_list",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/context/compression.py",
        "relativePath": "gpt_researcher/context/compression.py",
        "lineNo": 82,
        "endLineNo": 84,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fcontext%2Fcompression.py%23L82-L84&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis:\n\n**[Quick Summary]** \n\nThis function processes a list of documents (`docs`), extracts the title and content from each document using a `metadata.get('section_title')` and `page_content` attribute (assuming a custom document structure), and returns a truncated list of the top `top_n` documents represented as formatted strings. \n\n**[Inputs]**\n\n* `docs`: A list of document objects.\n* `top_n`: An integer representing the maximum number of documents to return.\n\n**[Output]**\n\n* A list of strings, where each string contains the extracted title and content of a document, formatted as \"Title: [title]\\nContent: [content]\".\n* The list will only contain the top `top_n` documents based on their order in the input `docs` list.  \n\n\n"
    },
    "gpt_researcher__document__document__DocumentLoader____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/document/document.py",
        "relativePath": "gpt_researcher/document/document.py",
        "lineNo": 17,
        "endLineNo": 19,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fdocument%2Fdocument.py%23L17-L19&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Please provide the code selection you would like me to analyze. \n\nI need to see the code to determine:\n\n* **Quick summary:** What the function does and its overall purpose.\n* **Inputs:** What data the function accepts and their meaning.\n* **Output:** What the function produces as a result. \n\n\nLet me know when you're ready! \ud83d\ude0a \n"
    },
    "gpt_researcher__document__langchain_document__LangChainDocumentLoader____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/document/langchain_document.py",
        "relativePath": "gpt_researcher/document/langchain_document.py",
        "lineNo": 12,
        "endLineNo": 14,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fdocument%2Flangchain_document.py%23L12-L14&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Analysis:\n\n**Quick Summary:** This line of code appears to be part of a class initialization. It assigns a list of `documents` to an instance variable called `self.documents`. This likely means the class is designed to work with a collection of documents and this line sets up the storage for those documents within an object of the class. \n\n**Inputs:**\n\n* `documents`: A list of documents (the exact type and structure of \"documents\" is unknown without context).\n\n**Output:**\n\n*  N/A - This line doesn't produce an output value. It modifies the state of the object. \n\n\n"
    },
    "gpt_researcher__llm_provider__generic__base__GenericLLMProvider____init__": {
        "label": "__init__",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/llm_provider/generic/base.py",
        "relativePath": "gpt_researcher/llm_provider/generic/base.py",
        "lineNo": 9,
        "endLineNo": 11,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fllm_provider%2Fgeneric%2Fbase.py%23L9-L11&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Please provide me with the code selection you'd like me to analyze!  I'm ready to give you a quick summary, inputs, and expected output. \ud83d\ude0a \n"
    },
    "gpt_researcher__master__agent__GPTResearcher__get_costs": {
        "label": "get_costs",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 411,
        "endLineNo": 413,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L411-L413&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "[Quick Summary] \nThis function, likely part of a larger program modeling research or development, calculates and returns the total costs associated with research activities. \n\n[Inputs]\n*  `self.research_costs`: This likely represents a variable within the object (instance of a class) holding a record or sum of all research costs.\n\n[Output]\n*  The total accumulated research costs. \n"
    },
    "gpt_researcher__master__agent__GPTResearcher__get_research_context": {
        "label": "get_research_context",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 408,
        "endLineNo": 410,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L408-L410&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Please provide the code selection you'd like me to analyze.  \n\n"
    },
    "gpt_researcher__master__agent__GPTResearcher__get_source_urls": {
        "label": "get_source_urls",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 405,
        "endLineNo": 407,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L405-L407&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Summary\n\nThis function retrieves a list of all URLs that have been visited by an object (likely a web crawler or similar). It provides a record of the URLs processed during the object's operation.\n\n## Inputs\n\n* `self.visited_urls`:  A data structure (likely a set or list) storing visited URLs, presumably managed within the object itself. \n\n## Output\n\n* A list containing all URLs stored in `self.visited_urls`. \n\n\n"
    },
    "gpt_researcher__master__agent__GPTResearcher__set_verbose": {
        "label": "set_verbose",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/agent.py",
        "relativePath": "gpt_researcher/master/agent.py",
        "lineNo": 414,
        "endLineNo": 416,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fagent.py%23L414-L416&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "You seem to have missed providing the code snippet! Please provide the code selection so I can analyze it and provide the summary, inputs, and output. \n\n"
    },
    "gpt_researcher__utils__validators__Subtopic": {
        "label": "Subtopic",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/utils/validators.py",
        "relativePath": "gpt_researcher/utils/validators.py",
        "lineNo": 5,
        "endLineNo": 7,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Futils%2Fvalidators.py%23L5-L7&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Code Analysis \n\n**Quick Summary:** This code defines a field named \"task\" within a data structure (likely part of a form or schema).  The field is intended to store a task name and must be at least one character long.  The purpose is to ensure data integrity by requiring a valid task name.\n\n**Inputs:**\n\n*  \"task\": A string representing the name of a task.\n\n\n**Output:**\n\n* A validated task name (string) that is at least one character long. \n"
    },
    "gpt_researcher__master__prompts__auto_agent_instructions": {
        "label": "auto_agent_instructions",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/prompts.py",
        "relativePath": "gpt_researcher/master/prompts.py",
        "lineNo": 163,
        "endLineNo": 164,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fprompts.py%23L163-L164&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Please provide the code selection you'd like me to analyze.  I'm ready to give you a quick summary, inputs, and output! \n"
    },
    "gpt_researcher__master__prompts__generate_report_introduction": {
        "label": "generate_report_introduction",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/prompts.py",
        "relativePath": "gpt_researcher/master/prompts.py",
        "lineNo": 338,
        "endLineNo": 339,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fprompts.py%23L338-L339&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Please provide the code selection you'd like me to analyze. I'm ready to give you a quick summary, list the inputs and outputs!  \n\n"
    },
    "gpt_researcher__master__prompts__generate_subtopics_prompt": {
        "label": "generate_subtopics_prompt",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/master/prompts.py",
        "relativePath": "gpt_researcher/master/prompts.py",
        "lineNo": 210,
        "endLineNo": 211,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmaster%2Fprompts.py%23L210-L211&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Please provide the code selection you would like me to analyze.  I need the code to give you a summary, inputs, and output. \ud83d\ude0a  \n"
    },
    "gpt_researcher__memory__embeddings__Memory__get_embeddings": {
        "label": "get_embeddings",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/memory/embeddings.py",
        "relativePath": "gpt_researcher/memory/embeddings.py",
        "lineNo": 36,
        "endLineNo": 37,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Fmemory%2Fembeddings.py%23L36-L37&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "Please provide the code selection you'd like me to analyze.  \n\nI'm ready to give you a quick summary, list the inputs and their meanings, and describe the expected output.  \n\n"
    },
    "gpt_researcher__utils__validators__Subtopics": {
        "label": "Subtopics",
        "systemPath": "/home/sanjay/Development/explore/gpt-researcher/gpt_researcher/utils/validators.py",
        "relativePath": "gpt_researcher/utils/validators.py",
        "lineNo": 8,
        "endLineNo": 9,
        "emgithubIframeLink": "https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fassafelovic%2Fgpt-researcher%2Fblob%2Fmaster%2Fgpt_researcher%2Futils%2Fvalidators.py%23L8-L9&style=default&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on",
        "description": "## Function Breakdown:\n\n**Quick Summary:**  This line initializes an empty list named `subtopics` to store objects of a class called `Subtopic`. This setup likely prepares a data structure to organize and manage subtopics within a larger topic or subject.\n\n**Inputs:** \n\n* None (The code only initializes a variable, it doesn't directly take input) \n\n**Output:**\n\n* An empty list named `subtopics` designed to hold `Subtopic` objects. \n"
    }
}